{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Differences between 08 notebook and 09\n",
        "\n",
        "* Still need to have meta data filters: https://github.com/run-llama/llama_index/issues/9334 (filter for citations or footnotes would be nice for some questions). Chroma could also include this too/instead.\n",
        "* Added a cleaner response format (basically json) that always includes justification.\n",
        "* A3 currently gives yes and section but really it should just be a yes or no and give justification.\n",
        "* D2 should incorporate whether there was crowdsourcing or not.\n",
        "* D4 should output whether there was approval or exempt.\n",
        "* Do not currently have node post processing (reranking or keyword specific including or excluding). This could fix prompt B1 and B3 (which specify). This was implemented previousl also shouldnt be too hard to include.\n",
        "* Still need to only have caption for figures (currently have more than just caption).\n",
        "\n",
        "* Learning from this book: https://mallahyari.github.io/rag-ebook/intro.html and Priyanshu's work"
      ],
      "metadata": {
        "id": "SXyqJOCgmDWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pylatexenc llama_index sentence-transformers llama-index-embeddings-huggingface pandas langchain pylatexenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJpRMbRH5Bem",
        "outputId": "83a0e2f2-3a61-43c6-f1db-032737721e88"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.10/dist-packages (2.10)\n",
            "Requirement already satisfied: llama_index in /usr/local/lib/python3.10/dist-packages (0.10.52)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.2.7)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.12)\n",
            "Requirement already satisfied: llama-index-core==0.10.52.post1 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.10.52.post1)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.10)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.2.3)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.25)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.26)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.1.6)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (1.35.9)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama_index) (1.14.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.11)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.83)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama_index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama_index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama_index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama_index) (1.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: minijinja>=1.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.1.2->llama_index) (0.4.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama_index) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama_index) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama_index) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama_index) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.52.post1->llama_index) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama_index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama_index) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama_index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core==0.10.52.post1->llama_index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama_index) (8.1.7)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.52.post1->llama_index) (1.7.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.52.post1->llama_index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core==0.10.52.post1->llama_index) (3.21.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.10.52.post1->llama_index) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Tw644kHaz51m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a27004-ce32-46e2-d8af-ddfbdb44b655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from pylatexenc.latex2text import LatexNodes2Text\n",
        "\n",
        "\n",
        "\n",
        "from llama_index.core.indices.query.query_transform.base import (\n",
        "    StepDecomposeQueryTransform,\n",
        ")\n",
        "from llama_index.core import Document\n",
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.extractors import (\n",
        "    SummaryExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        ")\n",
        "\n",
        "#\n",
        "from llama_index.core.node_parser import SimpleNodeParser # This defaults to SentenceSplitter basically anyways\n",
        "\n",
        "# Semantic Chunking: # https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/?h=tokentextsplitter#tokentextsplitter\n",
        "# Semantic similarity might be easier to work with so many prompts and with multiple pdf lengths ()\n",
        "from llama_index.core.node_parser import (\n",
        "    SentenceSplitter,\n",
        "    SemanticSplitterNodeParser,\n",
        ")\n",
        "\n",
        "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
        "from llama_index.core.postprocessor import LLMRerank\n",
        "\n",
        "\n",
        "# Current github issue here (rerank is still useful) :\n",
        "# https://github.com/run-llama/llama_index/issues/11093\n",
        "class SafeLLMRerank:\n",
        "    def __init__(self, choice_batch_size=5, top_n=2):\n",
        "        self.choice_batch_size = choice_batch_size\n",
        "        self.top_n = top_n\n",
        "        self.reranker = LLMRerank(\n",
        "            choice_batch_size=choice_batch_size,\n",
        "            top_n=top_n,\n",
        "        )\n",
        "\n",
        "    def postprocess_nodes(self, nodes, query_bundle):\n",
        "        try:\n",
        "            return self.reranker.postprocess_nodes(nodes, query_bundle)\n",
        "        except Exception as e:\n",
        "            print(f\"Rerank issue: {e}\")\n",
        "            return nodes\n",
        "\n",
        "# Need to modify so we can be sure it counts regular expression stuff\n",
        "from llama_index.core.postprocessor import KeywordNodePostprocessor\n",
        "\n",
        "\n",
        "\n",
        "from llama_index.core.node_parser import TokenTextSplitter # Should work better than just splitting by sentence\n",
        "\n",
        "# https://docs.llamaindex.ai/en/stable/examples/query_engine/pdf_tables/recursive_retriever/ (good tutorial for figuring things out and explaining in paper)\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.query_engine import MultiStepQueryEngine\n",
        "from llama_index.core.response.notebook_utils import display_source_node\n",
        "from llama_index.core.retrievers import RecursiveRetriever\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core import QueryBundle\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding # Need api key for this and we can change later if needed.\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "from llama_index.readers.file import PDFReader\n",
        "\n",
        "from llama_index.core.evaluation import (\n",
        "    generate_question_context_pairs,\n",
        "    EmbeddingQAFinetuneDataset,\n",
        ")\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# This is needed to have pydantic outputs in llama index in a nice structured format.\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from llama_index.core.schema import IndexNode, TextNode, NodeRelationship, RelatedNodeInfo\n",
        "\n",
        "from llama_index.core.embeddings import resolve_embed_model\n",
        "from google.colab import drive\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environmental Variables"
      ],
      "metadata": {
        "id": "BkfVPAEj6NMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is not necessary on our app (only did this because I need to redo my conda installation for llama index)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"TOGETHERAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "inyKC2ds1Maz"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data and Setup"
      ],
      "metadata": {
        "id": "jUHSxEBX6SBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tex_file = '/content/drive/MyDrive/Jobs/PhD/Research/Publications/EMNLP_2024_Demonstration/latex/FOMCacl2023.tex'\n",
        "#tex_file = '/content/drive/MyDrive/Jobs/PhD/Research/Publications/EMNLP_2024_Demonstration/latex/FiNER.tex'\n",
        "\n",
        "class SectionNumberer:\n",
        "    \"\"\"\n",
        "    When converting tex files to pdfs in overleaf, sections become numbered before appendix.\n",
        "    In the appendix, they are num\n",
        "    and during appendix they are lettered.\n",
        "    This function mimic that behavior by converting the sections\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.section_count = 0\n",
        "        self.subsection_count = 0\n",
        "        self.alpha_section_count = 0\n",
        "        self.bibliography_found = False  # Flag to track if bibliography has been found\n",
        "\n",
        "    def replace_heading(self, match):\n",
        "        command = match.group(1)  # 'section', 'subsection', or 'bibliography'\n",
        "        content = match.group(2)  # Title inside the braces\n",
        "\n",
        "        # If bibliography command is encountered, switch to alphabetic numbering\n",
        "        if command == 'bibliography':\n",
        "            self.bibliography_found = True\n",
        "            return match.group(0)  # Optionally return the bibliography line unchanged\n",
        "\n",
        "        # Process sections and subsections based on the numbering mode\n",
        "        if self.bibliography_found:\n",
        "            if command == 'section':\n",
        "                self.alpha_section_count += 1\n",
        "                section_label = chr(64 + self.alpha_section_count)  # Convert to letters A, B, C, etc.\n",
        "                self.subsection_count = 0  # Reset subsection count for new section\n",
        "                return f\"\\\\section{{{section_label} {content}}}\"\n",
        "            elif command == 'subsection':\n",
        "                self.subsection_count += 1\n",
        "                subsection_label = f\"{chr(64 + self.alpha_section_count)}.{self.subsection_count}\"\n",
        "                return f\"\\\\subsection{{{subsection_label} {content}}}\"\n",
        "        else:\n",
        "            if command == 'section':\n",
        "                self.section_count += 1\n",
        "                self.subsection_count = 0  # Reset subsection count\n",
        "                return f\"\\\\section{{{self.section_count} {content}}}\"\n",
        "            elif command == 'subsection':\n",
        "                self.subsection_count += 1\n",
        "                return f\"\\\\subsection{{{self.section_count}.{self.subsection_count} {content}}}\"\n",
        "\n",
        "    def number_sections(self, tex_content):\n",
        "        # Regex to find all section, subsection, or bibliography commands\n",
        "        pattern = re.compile(r\"\\\\(section|subsection|bibliography)\\{([^}]*)\\}\")\n",
        "        processed_content = pattern.sub(self.replace_heading, tex_content)\n",
        "        return processed_content\n",
        "\n",
        "def extract_text_and_captions(latex_string):\n",
        "    \"\"\"\n",
        "    Remove tables, but keep the table captions (they are numbered).\n",
        "    \"\"\"\n",
        "\n",
        "    # Regex to find all table environments (both \\begin{table*} and \\begin{table})\n",
        "    table_pattern = re.compile(r'(\\\\begin\\{table\\*?\\}.*?\\\\end\\{table\\*?\\})', re.DOTALL)\n",
        "\n",
        "    # Split the text at each table environment\n",
        "    parts = table_pattern.split(latex_string)\n",
        "\n",
        "    result_parts = []\n",
        "    caption_counter = 1\n",
        "\n",
        "    for part in parts:\n",
        "        if table_pattern.match(part):\n",
        "            # Find the caption within this table\n",
        "            caption_match = re.search(r'\\\\caption\\{([^}]*)\\}', part)\n",
        "            if caption_match:\n",
        "                caption_text = caption_match.group(1)\n",
        "                result_parts.append(f'Table {caption_counter} Description: {caption_text}. End Table {caption_counter} Description.')\n",
        "                caption_counter += 1\n",
        "        else:\n",
        "            result_parts.append(part)\n",
        "\n",
        "    # Combine the extracted text parts and captions\n",
        "    combined_text = ' '.join(result_parts)\n",
        "\n",
        "    # Clean up any extra spaces introduced\n",
        "    clean_text = re.sub(r'\\s+', ' ', combined_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "def read_latex_doc(tex_file):\n",
        "    \"\"\"\n",
        "    NOTE THAT FOR NOW, THE DOCUMENT WILL START WITH ABSTRACT AND WE CAN PARSE OUT TITLE AND AUTHOR NAMES LATER.\n",
        "    input: insert .tex document path\n",
        "\n",
        "    return: chunks\n",
        "    \"\"\"\n",
        "\n",
        "    with open(tex_file, 'r') as file:\n",
        "        tex_content = file.read()\n",
        "\n",
        "    def extract_title(tex_content):\n",
        "        \"\"\"\n",
        "        This is meant to go to the source in all nodes\n",
        "        \"\"\"\n",
        "\n",
        "        # Regex pattern to match text within \\title{...}\n",
        "        pattern = re.compile(r'\\\\title\\{([^}]*)\\}')\n",
        "        result = pattern.search(tex_content)\n",
        "        return result.group(1) if result else ''\n",
        "\n",
        "    def remove_document_tags(tex_content):\n",
        "        \"\"\"\n",
        "        Remove \\begin{document} and \\end{document} from LaTeX content.\n",
        "        \"\"\"\n",
        "        tex_content = re.sub(r'\\\\begin{document}', '', tex_content)\n",
        "        tex_content = re.sub(r'\\\\end{document}', '', tex_content)\n",
        "        return tex_content\n",
        "\n",
        "    def start_with_abstract(tex_content):\n",
        "        \"\"\"\n",
        "        Keep only the content starting from \\begin{abstract}.\n",
        "        \"\"\"\n",
        "        match = re.search(r'\\\\begin{abstract}', tex_content)\n",
        "        if match:\n",
        "            tex_content = tex_content[match.start():]\n",
        "        return tex_content\n",
        "\n",
        "    def remove_comments(tex_content):\n",
        "        \"\"\"\n",
        "        Remove commented lines from LaTeX content while preserving original line endings.\n",
        "        \"\"\"\n",
        "        lines = re.split('(\\r\\n|\\r|\\n)', tex_content)  # Capture the line endings\n",
        "        uncommented_lines = [line for line in lines if not line.strip().startswith('%') and not re.match(r'(\\r\\n|\\r|\\n)', line)]\n",
        "        line_endings = [line for line in lines if re.match(r'(\\r\\n|\\r|\\n)', line)]\n",
        "\n",
        "        # Reconstruct the text preserving line endings\n",
        "        uncommented_text = ''.join(uncommented_lines + line_endings)\n",
        "        return uncommented_text\n",
        "\n",
        "    def add_spaces_around_commands(text, commands):\n",
        "        for command in commands:\n",
        "            # Create a regular expression pattern for each command, including optional *\n",
        "            pattern = rf'(\\\\{command}\\*?\\{{.*?\\}})'\n",
        "\n",
        "            # Add spaces around each matched command pattern\n",
        "            text = re.sub(pattern, r' \\1 ', text)\n",
        "\n",
        "        # Remove any duplicate spaces that may have been introduced\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_consecutive_occurrences(line):\n",
        "        # Use a regular expression to replace consecutive occurrences of %%\n",
        "        # Some people use multiple line strings\n",
        "        return re.sub(r'(%%)+', r'\\1', line)\n",
        "\n",
        "    def number_sections(tex_content):\n",
        "        section_count = 0\n",
        "        appendix_mode = False\n",
        "        alpha_section_count = 0\n",
        "        subsection_count = 0  # Initialize subsection count\n",
        "\n",
        "        def replace_heading(match):\n",
        "            nonlocal section_count, alpha_section_count, appendix_mode, subsection_count\n",
        "            heading_type = match.group(1)  # Determine whether it's 'section' or 'subsection'\n",
        "            heading_content = match.group(2)  # Capture the title inside the braces\n",
        "\n",
        "            if \"\\\\appendix\" in heading_content:\n",
        "                appendix_mode = True\n",
        "                return match.group(0)  # Return the original line\n",
        "\n",
        "            if heading_type == 'section':\n",
        "                if appendix_mode:\n",
        "                    alpha_section_count += 1\n",
        "                    section_label = chr(64 + alpha_section_count)\n",
        "                    subsection_count = 0  # Reset subsection count\n",
        "                    return f\"\\\\section{{{section_label}. {heading_content}}}\"\n",
        "                else:\n",
        "                    section_count += 1\n",
        "                    subsection_count = 0  # Reset subsection count\n",
        "                    return f\"\\\\section{{{section_count}. {heading_content}}}\"\n",
        "            elif heading_type == 'subsection':\n",
        "                if appendix_mode:\n",
        "                    subsection_count += 1\n",
        "                    subsection_label = f\"{chr(64 + alpha_section_count)}.{subsection_count}\"\n",
        "                    return f\"\\\\subsection{{{subsection_label}. {heading_content}}}\"\n",
        "                else:\n",
        "                    subsection_count += 1\n",
        "                    return f\"\\\\subsection{{{section_count}.{subsection_count}. {heading_content}}}\"\n",
        "\n",
        "        # Regex to find all section and subsection commands\n",
        "        pattern = re.compile(r\"\\\\(section|subsection)\\{([^}]*)\\}\")\n",
        "        processed_content = pattern.sub(replace_heading, tex_content)\n",
        "        return processed_content\n",
        "\n",
        "    def split_sections(tex_content):\n",
        "        # Split using lookahead to ensure \\section starts a new chunk\n",
        "        # This splits before each \\section{...}\n",
        "        chunks = re.split(r'(?=\\\\section\\*?{[^}]*})', tex_content)\n",
        "\n",
        "        # Initialize list to store properly combined chunks\n",
        "        combined_chunks = []\n",
        "\n",
        "        # Append the first chunk directly as it includes content before any \\section\n",
        "        if chunks and not chunks[0].startswith('\\\\section'):\n",
        "            combined_chunks.append(chunks.pop(0))\n",
        "\n",
        "        # Remaining chunks should already start with \\section\n",
        "        combined_chunks.extend(chunks)\n",
        "\n",
        "        return combined_chunks\n",
        "\n",
        "    # Remove \\begin{document} and \\end{document}\n",
        "    tex_content = remove_document_tags(tex_content)\n",
        "\n",
        "    # List of LaTeX commands to handle that can add spaces where non exist. This is extremely important for llms to chunk.\n",
        "    commands = ['footnote', 'href', 'textbf', 'section', 'section*', 'subsection', 'subsection*']\n",
        "\n",
        "    tex_content = add_spaces_around_commands(tex_content, commands)\n",
        "\n",
        "    # Remove most of table content except caption.\n",
        "    tex_content = extract_text_and_captions(tex_content)\n",
        "\n",
        "    # Start with \\begin{abstract}\n",
        "    tex_content = start_with_abstract(tex_content)\n",
        "\n",
        "    # Remove commented lines\n",
        "    tex_content = remove_comments(tex_content)\n",
        "\n",
        "    # Remove multiple line comments.\n",
        "    tex_content = remove_consecutive_occurrences(tex_content)\n",
        "\n",
        "    # Create an instance of SectionNumberer and process the LaTeX content\n",
        "    numberer = SectionNumberer()\n",
        "    tex_content = numberer.number_sections(tex_content)\n",
        "\n",
        "    list_chunks = split_sections(tex_content)\n",
        "\n",
        "    # Regex pattern to match strings starting with \\section*{Acknowledgements} or \\section{Acknowledgements} (case-insensitive)\n",
        "    pattern = re.compile(r'\\\\section\\*?\\{acknowledgements\\}', re.IGNORECASE)\n",
        "\n",
        "    # Filter out items that match the pattern\n",
        "    list_chunks = [chunk for chunk in list_chunks if not pattern.match(chunk)]\n",
        "\n",
        "    # Replace \\begin{abstract} with \\section*{abstract}\n",
        "    list_chunks[0] = list_chunks[0].replace('\\\\begin{abstract}', '\\\\section*{abstract}')\n",
        "\n",
        "    # Replace \\end{abstract} with an empty string\n",
        "    list_chunks[0] = list_chunks[0].replace('\\\\end{abstract}', '')\n",
        "\n",
        "    # Extract the title content\n",
        "    title = extract_title(tex_content)\n",
        "\n",
        "    return(list_chunks, title)"
      ],
      "metadata": {
        "id": "3ZOWu98ebaln"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_chunks, title = read_latex_doc(tex_file)"
      ],
      "metadata": {
        "id": "VHpnuKwiDz1s"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne4KcfTHCZUT",
        "outputId": "8b3aec7e-43ea-4c0f-8667-61ad863c0796"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\\\section*{abstract} Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences financial markets. In this study, we develop a novel task of hawkish-dovish classification and benchmark various pre-trained language models on the proposed dataset. Using the best-performing model (RoBERTa-large), we construct a measure of monetary policy stance for the FOMC document release days. To evaluate the constructed measure, we study its impact on the treasury market, stock market, and macroeconomic indicators. Our dataset, models, and code are publicly available on Huggingface and GitHub under CC BY-NC 4.0 license \\\\footnote{The fine-tuned model and data are available on the \\\\href{https://huggingface.co/gtfintechlab/FOMC-RoBERTa} {Huggingface}. The code is available on \\\\href{https://github.com/gtfintechlab/fomc-hawkish-dovish} {FinTech Lab GitHub}.}.  ',\n",
              " '\\\\section{1 Introduction} On August 26th, 2022, FOMC Chair Jerome H. Powell gave an 8-minute long speech at Jackson Hole which immediately resulted in an almost \\\\$3 Trillion USD decline in U.S. equity market value that day. The speech was followed by more than \\\\$6 Trillion USD loss in equity valuation over the next 3 days. Drastic market shifts to the Fed\\'s pronouncements indicate just how important the FOMC communications have become and highlight the need for a model which can capture the policy stance from Fed-related text. The Federal Open Market Committee (FOMC) is a federal organization responsible for controlling U.S.\\'s open market operations and setting interest rates. It tries to achieve its two main objectives of price stability and maximum employment by controlling the money supply in the market. Given the market condition (employment rate and inflation), the Fed either increases (dovish), decreases (hawkish), or maintains the money supply \\\\footnote{Fed increases the money supply by lowering interest rates and decreases the money supply by increasing interest rates or by other means necessary. More detail on this can be found in the annotation guide. } (neutral). To understand the influence the FOMC has on the different financial markets, we need to extract its monetary policy stance and the corresponding magnitude from official communications. Utilizing the traditional sentiment analysis model, which classifies text into positive vs negative, one can\\'t extract policy stance. A sentence that has the word \"increase\" could either be dovish or hawkish without a clear negative connotation. For example, the word \"increase\" with the word \"employment\" means the economy is doing well, but the word \"increase\" with the word \"inflation\" is negative for the economy. Current SOTA finance domain-specific language models \\\\citep{orig_finbert, shah-etal-2022-flang} trained for sentiment analysis find both cases to be positive, which is inaccurate. The performance analysis for FinBERT \\\\citep{orig_finbert} model is provided in Appendix \\\\ref{ap:finbert_senti}. This problem creates a need to develop a new task for hawkish vs dovish classification accompanied by high-quality annotated data. Given the lack of annotated data, computational linguistic work related to FOMC text in the literature \\\\citep{rozkrut2007quest, zirn-etal-2015-lost, hansen2016shocking, rohlfs-etal-2016-effects, hansen2018transparency, nakamura2018high, cieslak2019stock, schmeling2019does, tsukioka2020tone, ehrmann2020starting, frunza-2020-information, gorodnichenko2021voice, matsui-etal-2021-using, 10.1145/3503161.3548380} so far has been limited to unsupervised and rule-based models. These rule-based models don\\'t perform well on the hawkish-dovish classification task, which we will use as a baseline in performance analysis. Additionally, we conduct a benchmark of the zero-shot ChatGPT model using the annotated dataset to gain insights into the significance of fine-tuning on such data. In this work, we collect text data (speech transcripts, press conference transcripts, and meeting minutes) from the FOMC over the period 1996-2022 and annotate a sample of each data type. We not only create new datasets and tackle the task of building a hawkish-dovish classifier, but also test the performance of various models starting from rule-based to fine-tuned large PLMs. As sentences presented in FOMC text sometimes have two sub-sentences that have counterfactual information to tone down the stance, we employ a simple sentence-splitting scheme as well. We also construct the aggregate monetary policy stance and show its validity by looking at its performance in predicting various financial market variables. Through our work, we contribute to the literature in the following way: \\\\begin{itemize} \\\\item We show that the traditional (rule-based) approach practiced in finance and economic literature is a rudimentary way to measure monetary policy stance from the text document. \\\\item We introduce a new task to classify sentences into hawkish vs dovish as opposed to positive vs negative sentence classification for monetary policy text. \\\\item We build comprehensive, clean, tokenized, and annotated open-source datasets for FOMC meeting minutes, press conferences, and speeches with detailed meta information. \\\\item We develop an aggregate monetary policy stance measure and validate its performance in predicting various economic and financial indicators. \\\\end{itemize} ',\n",
              " \"\\\\section{2 Related Work} \\\\paragraph{NLP in Finance} Over the last decade behind the evolution of NLP, there has been a growing literature on the applications of NLP techniques in Finance \\\\citep{loughran2011liability, sohangir2018big, xing2018natural, chava2022measuring}. The majority of the research takes advantage of news articles \\\\citep{vargas2017deep, caldara2022measuring}, SEC filings \\\\citep{loughran2011liability, chava2016december, alanis2022benchmarking}, or earnings conference calls \\\\citep{bowen2002conference, bushee2003open, chava2019buzzwords, li2020maec}. Development of finance domain-specific language models \\\\citep{orig_finbert, finbert, liu2020finbert} have pushed the current benchmarks further. Recent work of \\\\citet{shah-etal-2022-flang} proposes a set of heterogeneous benchmarks for the financial domain and shows SOTA performance using their proposed language model, but it doesn't include macroeconomics-based tasks. \\\\paragraph{FOMC and Text Analysis} A study on communications from the central banks of the Czech Republic, Hungary, and Poland by \\\\citet{rozkrut2007quest} suggests that words from central banks affect the market but the effect varies based on communication style. Other various studies \\\\citep{tobback2017between, hansen2018transparency, nakamura2018high, cieslak2019stock, schmeling2019does, tsukioka2020tone, ehrmann2020starting, bennani2020does, gorodnichenko2021voice} also point to a similar conclusion that the communication from the central banks moves the market, but they don't leverage the power of the transformer-based model available at their disposal. Many articles in the literature use LDA to analyze various texts released by Fed. \\\\citet{rohlfs-etal-2016-effects} uses LDA on the FOMC meeting statements to predict the fed fund rate and long-term treasury rate. \\\\citet{hansen2016shocking} use an LDA-based topic modeling on FOMC-released text to understand how forward guidance affects the market and economic variables. In their study, they only used statements released post-meeting and suggest that the use of meeting minutes and speeches may offer greater insight. \\\\citet{jegadeesh2017deciphering} also uses LDA to analyze meeting minutes. They suggest that even though meeting minutes are released a few weeks after the actual meeting, the minutes still carry pertinent market-moving information. In recent work by \\\\citet{10.1145/3503161.3548380}, they created a multimodal dataset (MONOPOLY) from video press conferences for multimodal financial forecasting. The MONOPOLY dataset is comprehensive and not only covers text but also utilizes audio and video features. Yet, it misses two critical economic downturn periods of the last two decades: The DotCom Bubble Burst of 2000-2002 and the Global Financial Crises of 2007-2008. \\\\citet{matsui-etal-2021-using} used word embedding to extract semantic changes in the monetary policy documents. \\\\citet{zirn-etal-2015-lost} used the graph clustering method to generate the hawkish-dovish stance of monetary policy due to the dearth of annotated data. \\\\citet{frunza-2020-information} developed an unsupervised methodology to extract various information from FOMC post-meeting statements. \",\n",
              " '\\\\section{3 Dataset} \\\\subsection{3.1 FOMC Data} The datasets we build are composed of three different types of data: meeting minutes, press conference transcripts, and speeches from the FOMC. Meeting minutes are defined as reports derived from the eight annually scheduled meetings of the FOMC. Press conference transcripts, meanwhile, are transcripts of the prepared remarks, followed by the Q\\\\&A session between the Federal Reserve chair and press reporters. Lastly, speeches were defined as any talk given by a Federal Reserve official. We limit our datasets to an end release date of October 15th, 2022, and attempt to collect as far back as possible for each category prior to this date. The meeting minutes and speeches spanned from a release period of January 1st, 1996 to October 15th, 2022. Press conferences are a more recent phenomenon and the data aggregated stretched from April 27th, 2011 to October 15th, 2022. We obtained the data by leveraging BeautifulSoup, Selenium, and manual downloading from \\\\url{http://www.federalreserve.gov/}. Regex tools were used to clean the data, which was stored in CSV or Excel format for processing. Sentence tokenization, using the library NLTK \\\\cite{bird2009natural} was done and datasets for each data category were initialized. \\\\paragraph{FOMC Raw Text Data} The overview of our initial raw text dataset is presented in Panel A of Table~\\\\ref{tb:raw_text_data_info}. Initial observations show that meeting minutes and speeches composed the bulk of our data, due to the recency of press conference transcripts. In addition, we also isolated only sentences where the speaker is designated as the Federal Reserve chair and the sentence was not a question in press conference transcripts, so this also served to reduce the data size. Across all forms of data, we had higher average words per sentence than the typical English language sentence, which averages 15 to 20 words \\\\cite{cutts2020oxford}. Our initial raw text data encompassed decades worth of crucial FOMC statements, however, a plethora of noise persisted in the data. Unrelated sentences riddled the datasets and a filter was needed to isolate key sentences relevant to changes in the federal reserve\\'s monetary policy stance. In addition, the number of sentences in the raw dataset was too vast to manually label, so a sampling procedure was needed. \\\\paragraph{Data \\\\& Title Filtration} As a result of data noise, a dictionary filter was developed to isolate sentences that would prove to be meaningful and allow us to determine monetary policy stance. The criteria for the filter was based on the dictionary developed by \\\\citet{gorodnichenko2021voice}. Any sentence that contained an instance of the words outlined in panel A1 or B1 in Table \\\\ref{tb:rule-based} would be kept, while anything else would be filtered out. The sentences kept were considered \"target\" sentences or textual data that we consider pertinent and later used to sample from and annotate. Table 1 Description: Rule-based dictionary used by \\\\citeauthor{gorodnichenko2021voice. End Table 1 Description. Our dictionary filter was also applied to speech data. Speech data was the largest dataset derived from web scraping, however, speeches contained the most noise, owing to many non-monetary policy speeches. Unlike the meeting minutes and press conference transcripts, speech data was accompanied with a title, so to isolate only relevant FOMC speeches to sample from, we applied the dictionary filter discussed in Table \\\\ref{tb:rule-based} onto the title of each speech. We justify this procedure in Table \\\\ref{tb:speech-filter} as this methodology results in the greatest \"target\" sentence per file. Overall, the filtration process isolated relevant files and \"target\" sentences in our raw data and set the stage for later sampling. The filter\\'s impact on the raw data is presented in Panel B of Table \\\\ref{tb:raw_text_data_info}. Table 2 Description: Details on the speech title filter methodology. End Table 2 Description. %% Table 3 Description: Details on the text data covered from FOMC. End Table 3 Description. %% \\\\paragraph{Sampling and Manual Annotation} As our data was unlabeled, our analysis necessitated the usage of manual labeling. To efficiently develop a manually labeled dataset, sampling was required. Our sampling procedure was to extract 5 random sentences and compile a larger data set. If fewer than 5 sentences were present in the file, all sentences were added. This sampling procedure resulted in a 1,070-sentence Meeting Minutes dataset, a 315-sentence Press Conference dataset, and a 994-sentence Speech dataset. For the labeling process, sentences were categorized into three classes (0: Dovish, 1: Hawkish, and 2: Neutral). We annotate each category of the data as a model trained on various categories as a model trained on the same category of data does not perform optimally. We provide evidence for this claim in Appendix \\\\ref{ap:transfer_learning}. % \\\\\\\\ % $sentence_i = $ $\\\\begin{cases} % 0 & \"Dovish\" \\\\\\\\ % 1 & \"Hawkish\" \\\\\\\\ % 2 & \"Neutral\" % \\\\end{cases}$ % \\\\\\\\ \\\\\\\\ Dovish sentences were any sentence that indicates future monetary policy easing. Hawkish sentences were any sentence that would indicate a future monetary policy tightening. Meanwhile, neutral sentences were those with mixed sentiment, indicating no change in the monetary policy, or those that were not directly related to monetary policy stance. The labeling was conducted by two different annotators and done independently to reduce potential labeling bias. Each annotator\\'s labeling was compared against each other and validated to ensure the consistency of the labeling results. The detail on the annotation agreement is provided in Appendix \\\\ref{sec:agreement_ann}. The labeling was conducted according to a predefined annotation guide, which is provided in Appendix \\\\ref{sec:appendix_manual_ann}. The guide is broken down into key sections such as economic status, dollar value change, energy/house prices, future expectations, etc. \\\\paragraph{Sentence Splitting} A common occurrence in the labeling process was the existence of intentional mixed tone. The Federal Reserve by purpose serves to maintain financial/economic stability and any statement they make is projected in a moderating manner to reduce the chance of excess market reaction. As a result, the Fed is known to project a stance but often accompanies this with a moderating statement that serves as a counterweight to the original stance. This produces a greater occurrence of neutral sentences. To address this possibility, we instituted sentence splitting to separate the differing stances. Initially, we implemented the lexicon-based package SentiBigNomics \\\\citep{SentiBigNomics} for sentence splitting, but it resulted in poor performance, causing us to pivot our approach. We developed a custom sentence-splitting method based on keywords. In Fed statements, the counter-statements are produced after a connective contrasting word. We carried sentence splits at the presence of the following keywords in a given statement: \"but\", \"however\", \"even though\", \"although\", \"while\", \";\". A sentence split was valid if each split segment contained a key word present in Table \\\\ref{tb:rule-based}. Statistics on the dataset before and after splitting are provided in Table \\\\ref{tb:before_after_split}. Table 4 Description: Number of sentences in the labeled dataset before and after splitting for each event. . End Table 4 Description. \\\\subsection{3.2 Economic Data} \\\\paragraph{CPI and PPI} We collect Consumer Price Index (CPI) data, and Producer Price Index (PPI) data from FRED \\\\footnote{\\\\url{https://fred.stlouisfed.org} }. The data is available at the monthly frequency for the first day of each month. Throughout the paper, we use percentage change from last year as CPI and PPI inflation measures. \\\\paragraph{US Treasury} We collect US treasury yield data for different maturities from the U.S. Department of the Treasury \\\\footnote{\\\\url{https://home.treasury.gov} }. It provides a daily yield of bonds for various maturities. \\\\paragraph{QQQ Index} We collect the adjusted closing index price of QQQ from Yahoo Finance \\\\footnote{\\\\url{https://finance.yahoo.com/quote/QQQ/history?p=QQQ} }. It contains daily QQQ index data since March 9, 1999. ',\n",
              " '\\\\section{4 Models} \\\\subsection{4.1 Rule-Based} In financial literature, rule-based classification has been the norm. Many of these rule-based systems work by classifying based on the presence of a combination of keywords. \\\\citet{gorodnichenko2021voice} in particular highlighted the effectiveness of this approach by classifying sentences as dovish or hawkish based on the combination of financial-related nouns and verbs in set panels in a given sentence. We have applied \\\\citeauthor{gorodnichenko2021voice}\\'s financial word dictionary rule-based approach to our developed datasets. In Table \\\\ref{tb:rule-based}, a sentence is considered dovish if it contains words present in panels A1 and A2 or B1 and B2. Otherwise, if it contains words present in A1 and B2 or A2 and B1 are considered hawkish. If a given sentence contains a word from panel C we reverse our initial classification, so dovish becomes hawkish and vice versa. We aim to capture and measure the effectiveness of the rule-based approach against our dataset to provide a benchmark against the deep learning models we apply later. We apply this rule-based approach on testing datasets that we derive from each dataset on an 80:20 training-test split. \\\\subsection{4.2 LSTM \\\\& Bi-LSTM} Long short-term memory (LSTM) is a recurrent neural network structure utilized for classification problems. The Bi-LSTM is a variation of an LSTM, which takes input bidirectionally. We apply both an LSTM and a Bi-LSTM to our developed datasets to gauge the effectiveness of RNNs in monetary stance classification. We instituted an 80:20 training-validation split to derive our initial training and validation datasets. A vocabulary was developed for both models against the training dataset for the purpose of vectorization. The encoding process worked by first initializing a tokenizer that eliminated all punctuation, normalized all sentences to lowercase, and splits sentences into word tokens. We limit the vocabulary size to 2,000 and any words outside the vocabulary were replaced with a placeholder token. A vocabulary size of 2,000 covers more than 99\\\\% of words in MM and PC text and covers around 91\\\\% in SP text. The lower coverage for speeches is due to the wide variety of miscellaneous topics outside of the scope of monetary policy. Our vocabulary allowed us to convert each sentence into a word vector by mapping each word to a corresponding numerical value present in the dictionary. Each word vector size was set to the length of the longest sentence present in the training dataset, and padding was done to meet the required vector size. We applied this encoding process to the training, testing, and validation datasets. Upon the complexion of vectorization, the word vectors were passed into our single-layer LSTM (32,379 parameters) and single-layer Bi-LSTM (32,735 parameters) models. Masking was also configured to true to ignored padded data and dropout was added to reduce potential over-fitting. We ran each model at varying epochs (10, 20, 30) and batch sizes (4, 8, 16, 32). Implementation of models was done using Tensorflow \\\\citep{tensorflow} on an NVIDIA RTX A6000 GPU. %The best-hyper-parameters for each dataset are displayed. \\\\subsection{4.3 PLMs} To set a benchmark, we include a range of small and large transformer-based models in our study. For small models, we use BERT \\\\citep{bert}, FinBERT \\\\citep{finbert}, FLANG-BERT \\\\citep{shah-etal-2022-flang}, FLANG-RoBERTa \\\\citep{shah-etal-2022-flang}, and RoBERTa \\\\citep{roberta}. In the large model category, we include BERT-large \\\\citep{bert} and RoBERTa-large \\\\citep{roberta}. We do not perform any pre-training on these models before employing them for fine-tuning to avoid overfitting on FOMC text. For each model, we find best hyper-parameters by performing a grid search on four different learning rates (1e-4, 1e-5, 1e-6, 1e-7) and four different batch sizes (32, 16, 8, 4). We conduct all experiments using PyTorch \\\\citep{pytorch} on NVIDIA RTX A6000 GPU. Each model was initialized with the pre-trained version on the Transformers library of Huggingface~\\\\citep{huggingface}. \\\\subsection{4.4 ChatGPT} In order to provide the performance benchmark of the current SOTA generative LLM, we measure the zero-shot performance of ChatGPT. We use the \"gpt-3.5-turbo\" model with 1000 max tokens for output, and a 0.0 temperature value. All the API calls were made on either May 3rd, 2023, or May 4th, 2023. We use the following zero-shot prompt: \"Discard all the previous instructions. Behave like you are an expert sentence classifier. Classify the following sentence from FOMC into `HAWKISH\\', `DOVISH\\', or `NEUTRAL\\' class. Label `HAWKISH\\' if it is corresponding to tightening of the monetary policy, `DOVISH\\' if it is corresponding to easing of the monetary policy, or `NEUTRAL\\' if the stance is neutral. Provide the label in the first line and provide a short explanation in the second line. The sentence: \\\\{sentence\\\\}\" Table 5 Description: Here MM indicates that the annotated dataset on meeting minutes is used for training and testing hawkish vs dovish task. Similarly, PC stands for press conference data, SP stands for speech data, and Combined is combined data of MM, SP, and PC. *-S indicates the version of the dataset after splitting sentences and reannotation. All values are F1 scores. An average of 3 seeds was used for all models. The standard deviation of F1 scores is reported in parentheses on the next line. ChatGPT and rule-based models are tested as zero-shot while all other models are fine-tuned with training data. . End Table 5 Description. \\\\begin{figure*}[ht] \\\\centering \\\\begin{subfigure}{0.9\\\\linewidth} \\\\centering \\\\includegraphics[width=\\\\linewidth]{images/plot_CPI.png} \\\\caption{} \\\\label{fig:CPI_measure} \\\\end{subfigure} \\\\hfill \\\\\\\\[\\\\baselineskip] \\\\begin{subfigure}{0.9\\\\linewidth} \\\\centering \\\\includegraphics[width=\\\\linewidth]{images/plot_PPI.png} \\\\caption{} \\\\label{fig:PPI_measure} \\\\end{subfigure} \\\\centering \\\\caption{(a) Our measure on meeting release date and 1-year change in CPI data on the first day of each month (b) Our measure on meeting release date and 1-year change in PPI data on the first day of each month} \\\\label{fig:CPI_PPI_measure} \\\\end{figure*} Table 6 Description: Correlation of immediate next CPI and PPI data with our measure. All values are statistically significant. The value in parentheses represents the corresponding p-value. CPI and PPI are the percentage change from last year. . End Table 6 Description. ',\n",
              " '\\\\section{5 Results and Analysis} In this section, we evaluate and benchmark different NLP models on the hawkish vs dovish classification task that we created. For all models and datasets, we used training and testing data based on an 80:20 split. Upon this split, we institute another 80:20 split on the training data to generate our final training and validation data. We use the best-performing model (RoBERTa) to generate a document (event) level measure of hawkish tone. We then validate the generated measure by looking at its relation with the inflation indicators and the US treasury. We also look at the performance of a simple trading strategy based on the generated measure. \\\\subsection{5.1 Model Performance} We ran all models listed in the previous section on three different categories and combined data. For each dataset, we train and test each model on both the before-split and after-split versions of sentences. For each model, we use three different seeds (5768, 78516, 944601) and calculate the average weighted F1 scores. The results for best hyper-parameters are listed in Table~\\\\ref{tb:master_accuracy}. \\\\paragraph{Rule-Based} As expected the rule-based model doesn\\'t perform very well. The rule-based approach optimizes the time needed for classification, but sacrifices the nuance of complex sentences, which necessitate context. It gives an F1 score of around 0.5 for nearly all datasets. The method sets a good baseline for the dataset as it\\'s still widely used in econ literature. \\\\paragraph{LSTM \\\\& Bi-LSTM} Although the LSTM and Bi-LSTM models are able to utilize greater context for classification, they did not perform significantly better than the initial rule-based approach. As seen across all data categories, the RNN models performed marginally the same. The LSTM and Bi-LSTM performances largely differed between the data categories. They performed worst when applied to the press conference datasets, a discrepancy caused by the small size of the dataset. In fact, in the smaller press conference datasets, the rule-based performed better than the expected RNN approach. Unlike rule-based approaches, neural network classification requires a large database to train from to improve accuracy. Concurrently, the recurrent neural networks worked best when applied to the meeting minutes and speech datasets. When compared against all data categories, the Bi-LSTM did not perform significantly better than the LSTM itself. The RNNs are effective in sentence classification, yet their limited success with FOMC sentences demonstrates the need for a transformer-based model. \\\\paragraph{PLMs} Finetuned PLMs outperform rule-based model and LSTM models by a significant margin. In base size, RoBERTa-base outperforms all other models on all datasets except after-split meeting minutes data (MM-S). On PC, FLANG-RoBERTa performs best. A future study using ablation states of models to understand why the finance domain-specific language models don\\'t outperform RoBERTa and how they can be improved could be fruitful. In large category and overall, RoBERTa large provide the best performance across all categories except PC-S. We note that sentence splitting does help improve performance for meeting minutes and press conference data, but it doesn\\'t help with speech data. Also, on average improvement from sentence splitting is higher with the base models compared to large models. The goal of sentence splitting is to not improve the performance of the classification task but to better measure, the document-level monetary policy stance constructed in the next section. In order to make sure that there is no look-ahead bias in our performance, we perform a robustness check in Appendix \\\\ref{ap:robust}. \\\\paragraph{ChatGPT} Zero-shot ChatGPT outperforms both rule-based and fine-tuned RNN-based (LSTM \\\\& Bi-LSTM) models. We note that the ChatGPT can\\'t be considered a good baseline as it has many issues highlighted by \\\\citet{rogers-etal-2023-closed}. ChatGPT model with zero-shot underperforms fine-tuned PLMs across all datasets. The finding here is in line with the survey done by \\\\citet{pikuliak_chatgpt_survey}, which finds that zero-shot ChatGPT fails to outperform fine-tuned models on more than 77\\\\% of NLP tasks. \\\\subsection{5.2 Hawkish Measure Construction} We use the RoBERTa-large model finetuned on the combined data to label all the filtered sentences in the meeting minutes, speeches, and press conferences. We then use labeled sentences in each document to generate a document-level measure of hawkishness for document $i$ using the following formula: \\\\begin{equation*} Measure_{i} = \\\\frac{\\\\#Hawkish_{i} - \\\\#Dovish_{i}}{\\\\#Total_{i}} \\\\end{equation*} where $Measure_{i}$ is document level measure, $\\\\#Hawkish_{i}$ is number of hawkish sentences in document $i$, $\\\\#Dovish_{i}$ is number of dovish sentences in document $i$, and $\\\\#Total_{i}$ is the total number of filtered sentences. \\\\subsection{5.3 Market Analysis} \\\\paragraph{Our Measure with CPI and PPI} To understand how quick the Fed is in reacting to inflation or deflation we use monthly CPI and PPI data and overlay our measure. As observed in Figure~\\\\ref{fig:CPI_PPI_measure}, our measure based on meeting minutes captures both the inflation and deflation period pretty well. It also shows that when Fed reacts quickly (2001 and 2008) it controls inflation and deflation better. We also look at the correlation of our measure with the CPI and PPI percentage change. As reported in Table~\\\\ref{tb:CPI_PPI_corr}, for all three data classes we find a statistically significant positive correlation. We also observe that the correlation increases over time as Fed is communicating its policy stance better to the public in recent years. As part of better communication, the Fed has started hosting press conferences at every alternate meeting starting in 2011 and every meeting starting in 2019. We refer readers to \\\\citet{coibion2022monetary} for a detailed discussion on Fed communication shift over time. Table 7 Description: Regression of Treasury yield with different maturity on our measure. All values are statistically significant. *, **, and *** indicate significance at the 10\\\\%, 5\\\\%, and 1\\\\% levels, respectively. . End Table 7 Description. \\\\paragraph{US Treasury Market} is highly sensitive to monetary policy changes. We validate the power of our measure in estimating treasury yield by running the linear regression provided in the Eq~\\\\ref{eq:treasury_reg}. We run the regression for three different maturities (3 months, 1 year, and 10 years) using three time-series measures generated from meeting minutes, speeches, and testimonies. We report the results in Table~\\\\ref{tb:treasury_reg}. We observe that the yield of treasury with 1-year maturity is most sensitive to monetary policy changes. All the regression yields statistically significant results which further validate the generated measure. \\\\begin{equation} Yield_{t,T} = \\\\alpha_{T} + \\\\beta_{T} * Measure_{t} + \\\\epsilon_{t,T} \\\\label{eq:treasury_reg} \\\\end{equation} here $T$ indicates maturity, and $t$ indicates the date on which the document was released. \\\\begin{figure}[ht] \\\\centering \\\\includegraphics[width=\\\\linewidth]{images/plot_trading_whitespace.png} \\\\caption{Value of \\\\$100 portfolio over time for two different strategies (Buy and Hold, and Our Strategy).} \\\\label{fig:trading_qqq} \\\\end{figure} \\\\paragraph{Equity Market} For a reality check, we construct a simple trading strategy based on the generated measure and compare its performance against the \"Buy and Hold\" strategy. In our strategy, we take a short position of the QQQ index fund when the measure is positive (hawkish) and a long QQQ position when the measure is negative (dovish). In the \"Buy and Hold\" strategy, the portfolio is always long QQQ. As shown in Figure~\\\\ref{fig:trading_qqq}, our strategy provides an excess return of 163.4\\\\% (673.29\\\\% our strategy vs 509.89\\\\% buy and hold) compared to the buy and hold strategy as of September 21st, 2022. Not only did our strategy outperform at the end, but it gives a better return during the majority of the period. We analyze the strategy for the period for which we have press conference data available. We choose press conference data because it is available immediately after the meeting as opposed to meeting minutes which are released after at least 21 days. ',\n",
              " '\\\\section{6 Conclusion} Our work contributes a new cleaned, tokenized, and labeled open-source dataset for FOMC text analysis of various data categories (meeting minutes, speeches, and press conferences). We also propose a new sequence classification task to classify sentences into different monetary policy stances (hawkish, dovish, and neutral). We show the application of this task by generating a measure from the trained model. We validate the measure by studying its relation with CPI, PPI, and Treasury yield. We also propose a simple trading strategy that outperforms the high benchmark set by the QQQ index over the last decade. We release our models, code, and benchmark data on Hugging Face and GitHub. We also note that the trained model for monetary policy stance classification can be used on other FOMC-related texts. ',\n",
              " \"\\\\section*{Limitations} In this article, we focus only on meeting minutes, speech, and press conference data. Many other text datasets such as transcripts from congressional and senate testimonies, beige books, green books, etc can be incorporated to understand pre-FOMC drift better. We don't use audio or video features in constructing the measure, which might contain additional information. It can be an interesting future study to compare measures generated from FOMC text with an alternate measure that can be constructed from the news or social media data. In dataset construction, while splitting sentences, we use a simple rule-based approach. We leave it as an open problem for future researchers to find better methods for splitting sentences with opposite tones. In our trading strategy construction, we do not include transaction fees as it involves low-frequency trading. In the future, one can use our model and data to construct a high-frequency trading strategy as well. In addition, a more comprehensive zero-shot and few-shot generative LLM benchmark with open-source models can be performed to provide a better comparison. \",\n",
              " \"\\\\section*{Ethics Statement} We acknowledge the geographic bias in our study as we only study the data from the Federal Reserve Bank of the United States of America. We also recognize the presence of gender bias in our study, given the Fed had a female chair for only 4 years out of 27 years (actually the only female chair in its entire history) of the observation period. Data used in the study which will be made public doesn't pose any ethical concerns as all the raw data is public and Fed is subject to public scrutiny. All of the language models used are publicly available and under the license category that allows us to use them for our purpose. Given the pre-training of large PLMs has a big carbon footprint, we limit our work to fine-tuning the existing PLMs. \",\n",
              " '\\\\section{A FinBERT Sentiment Analysis} \\\\label{ap:finbert_senti} In order to objectively understand the necessity of the new task and the created dataset, we use the \\\\href{https://huggingface.co/ipuneetrathore/bert-base-cased-finetuned-finBERT} {fine-tuned model} available on Hugging-face. The model is fine-tuned for financial sentiment analysis using the pre-trained FinBERT \\\\citep{orig_finbert}. We associate the \"positive\" label of FinBERT with \"dovish\", \"negative\" label with \"hawkish\", and \"neutral\" with \"neutral\" to measure the zero-shot performance on our dataset. The results in Table \\\\ref{tb:finbert_senti} show that the model doesn\\'t perform well, thus reemphasizing the need for a new dataset and task for hawkish-dovish classification. Table 8 Description: Here MM indicates that the annotated dataset on meeting minutes is used for training and testing hawkish vs dovish tasks. Similarly, PC stands for press conference data, SP stands for speech data, and Combined is combined data of MM, SP, and PC. *-S indicates the version of the dataset after splitting sentences and reannotation. All values are F1 scores. 3 seeds were used for all datasets.. End Table 8 Description. ',\n",
              " '\\\\section{B Transfer Learning} \\\\label{ap:transfer_learning} To understand if there is a need to annotate all three categories of data or whether the model trained on two categories of data can do equally well on the third category, we run an additional experiment. Here we take our best-performing (RoBERTa-large) model and train it on the train split of meeting minutes and press conference combined data and test it on a test sample of speech data. We additionally perform a grid search on four different learning rates (1e-4, 1e-5, 1e-6, 1e-7) and four different batch sizes (32, 16, 8, 4) to find the best hyperparameters. The best average F1 score for 3 seeds is 0.6625 which is lower compared to 0.7169 for the model trained on a training sample of speech data. ',\n",
              " '\\\\section{C Manual Annotation} \\\\subsection{C.1 Annotation Agreement} \\\\label{sec:agreement_ann} Annotation agreement statistics for the split categories of the dataset are provided in Table \\\\ref{tb:annotation_agreement}. Any disagreement between the two annotators was resolved using the annotation guide. If the annotation guide did not cover a specific case of disagreement, online resources were used and the missing case was later added to the annotation guide. Table 9 Description: Annotation agreement statistics for various categories of datasets. . End Table 9 Description. \\\\subsection{C.2 Annotation Guide} Table 10 Description: Annotation Guide. End Table 10 Description. Our annotation guide was built by dividing each target sentence into eight defined categories: Economic Status, Dollar Value Change, Energy/House Prices, Foreign Nations, Fed Expectations/Actions/Assets, Money Supply. \\\\begin{itemize} \\\\item \\\\emph{Economic Status}: A sentence pertaining to the state of the economy, relating to unemployment and inflation \\\\item \\\\emph{Dollar Value Change}: A sentence pertaining to changes such as appreciation or depreciation of value of the United States Dollar on the Foreign Exchange Market \\\\item \\\\emph{Energy/House Prices}: A sentence pertaining to changes in prices of real estate, energy commodities, or energy sector as a whole. \\\\item \\\\emph{Foreign Nations}: A sentence pertaining to trade relations between the United States and a foreign country. If not discussing United States we label neutral. \\\\item \\\\emph{Fed Expectations/Actions/Assets}: A sentence that discusses changes in the Fed yields, bond value, reserves, or any other financial asset value. \\\\item \\\\emph{Money Supply}: A sentence that overtly discusses impact to the money supply or changes in demand. \\\\item \\\\emph{Key Words/Phrases}: A sentence that contains key word or phrase that would classify it squarely into one of the three label classes, based upon its frequent usage and meaning among particular label classes. \\\\item \\\\emph{Labor}: A sentence that relates to changes in labor productivity \\\\end{itemize} A label of \"Dovish\", \"Hawkish\", and \"Neutral\" were assigned based on the contents of each sentence by category. The annotation guide and categories were influenced by initial readings of FOMC text and the need to maintain a consistent labeling standard. The annotation guide was utilized during the labeling procedure by two independent annotators to classify each sentence. Both annotators were male researchers, who have taken finance-related coursework and understood macroeconomics. One originated from the United States, while the other was from India. \\\\label{sec:appendix_manual_ann} ',\n",
              " '\\\\section{D Robustness check} \\\\label{ap:robust} As our dataset is a temporal dataset and the RoBERTa model is trained on data available prior to mid-2019, our model could have utilized future knowledge to predict past sentences a phenomenon deemed \"look-ahead bias\". Our train-test split based on different seeds contains this bias, so to ensure that it is not present in our model performance, we perform a robustness check by generating a train-test split based on time and checking the performance of the best-performing (RoBERTa-large) model. We split the Combined-S data into a training set spanning from 1996 to 2019 and a test set from 2020 to 2022. For the experiment, we averaged our model performance across 3 seeds (5768, 78516, 944601) and generated an average weighted F1 score of 0.7114, thus validating our performance as not being driven by look-ahead bias.']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_chunks[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "hT2lS5NdxDth",
        "outputId": "81bcd2c0-3011-4723-b444-4f08312caf74"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\\\section{2 Related Work} \\\\paragraph{NLP in Finance} Over the last decade behind the evolution of NLP, there has been a growing literature on the applications of NLP techniques in Finance \\\\citep{loughran2011liability, sohangir2018big, xing2018natural, chava2022measuring}. The majority of the research takes advantage of news articles \\\\citep{vargas2017deep, caldara2022measuring}, SEC filings \\\\citep{loughran2011liability, chava2016december, alanis2022benchmarking}, or earnings conference calls \\\\citep{bowen2002conference, bushee2003open, chava2019buzzwords, li2020maec}. Development of finance domain-specific language models \\\\citep{orig_finbert, finbert, liu2020finbert} have pushed the current benchmarks further. Recent work of \\\\citet{shah-etal-2022-flang} proposes a set of heterogeneous benchmarks for the financial domain and shows SOTA performance using their proposed language model, but it doesn't include macroeconomics-based tasks. \\\\paragraph{FOMC and Text Analysis} A study on communications from the central banks of the Czech Republic, Hungary, and Poland by \\\\citet{rozkrut2007quest} suggests that words from central banks affect the market but the effect varies based on communication style. Other various studies \\\\citep{tobback2017between, hansen2018transparency, nakamura2018high, cieslak2019stock, schmeling2019does, tsukioka2020tone, ehrmann2020starting, bennani2020does, gorodnichenko2021voice} also point to a similar conclusion that the communication from the central banks moves the market, but they don't leverage the power of the transformer-based model available at their disposal. Many articles in the literature use LDA to analyze various texts released by Fed. \\\\citet{rohlfs-etal-2016-effects} uses LDA on the FOMC meeting statements to predict the fed fund rate and long-term treasury rate. \\\\citet{hansen2016shocking} use an LDA-based topic modeling on FOMC-released text to understand how forward guidance affects the market and economic variables. In their study, they only used statements released post-meeting and suggest that the use of meeting minutes and speeches may offer greater insight. \\\\citet{jegadeesh2017deciphering} also uses LDA to analyze meeting minutes. They suggest that even though meeting minutes are released a few weeks after the actual meeting, the minutes still carry pertinent market-moving information. In recent work by \\\\citet{10.1145/3503161.3548380}, they created a multimodal dataset (MONOPOLY) from video press conferences for multimodal financial forecasting. The MONOPOLY dataset is comprehensive and not only covers text but also utilizes audio and video features. Yet, it misses two critical economic downturn periods of the last two decades: The DotCom Bubble Burst of 2000-2002 and the Global Financial Crises of 2007-2008. \\\\citet{matsui-etal-2021-using} used word embedding to extract semantic changes in the monetary policy documents. \\\\citet{zirn-etal-2015-lost} used the graph clustering method to generate the hawkish-dovish stance of monetary policy due to the dearth of annotated data. \\\\citet{frunza-2020-information} developed an unsupervised methodology to extract various information from FOMC post-meeting statements. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing Documents into Text Chunks (Nodes)"
      ],
      "metadata": {
        "id": "ALR2C6rhQlWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(text):\n",
        "    # Regex pattern to match text within curly braces for all specified cases\n",
        "    pattern = re.compile(r'\\\\(?:begin|section\\*?)\\{([^}]*)\\}')\n",
        "    result = pattern.search(text)\n",
        "    return result.group(1) if result else ''\n",
        "\n",
        "def check_license(node1):\n",
        "    \"\"\"\n",
        "    This is just an experiment with metadata for licenses\n",
        "    \"\"\"\n",
        "    normalized_node = node1.lower().replace('-', ' ')\n",
        "    if 'cc by nc 4.0' in normalized_node:\n",
        "        return 'CC BY-NC 4.0'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "base_nodes = []\n",
        "for chunk in list_chunks:\n",
        "    # This code is here in case we have duplicate section or Abstract names\n",
        "    node_id = extract_text(chunk)\n",
        "    base_nodes.append(TextNode(text=chunk, id_=node_id, metadata = {'license': check_license(chunk)}))\n",
        "\n",
        "# Add relationships between nodes\n",
        "for i, node in enumerate(base_nodes):\n",
        "    if i < len(base_nodes) - 1:\n",
        "        next_node = base_nodes[i + 1]\n",
        "        node.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(\n",
        "            node_id=next_node.id_\n",
        "        )\n",
        "    if i > 0:\n",
        "        previous_node = base_nodes[i - 1]\n",
        "        node.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(\n",
        "            node_id=previous_node.id_\n",
        "        )"
      ],
      "metadata": {
        "id": "6_F8Hmsp9MZ1"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_nodes[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP823GC2s7id",
        "outputId": "389e38d2-b882-406d-aeac-5f189a8fc03d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextNode(id_='abstract', embedding=None, metadata={'license': 'CC BY-NC 4.0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1 Introduction', node_type=None, metadata={}, hash=None)}, text='\\\\section*{abstract} Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences financial markets. In this study, we develop a novel task of hawkish-dovish classification and benchmark various pre-trained language models on the proposed dataset. Using the best-performing model (RoBERTa-large), we construct a measure of monetary policy stance for the FOMC document release days. To evaluate the constructed measure, we study its impact on the treasury market, stock market, and macroeconomic indicators. Our dataset, models, and code are publicly available on Huggingface and GitHub under CC BY-NC 4.0 license \\\\footnote{The fine-tuned model and data are available on the \\\\href{https://huggingface.co/gtfintechlab/FOMC-RoBERTa} {Huggingface}. The code is available on \\\\href{https://github.com/gtfintechlab/fomc-hawkish-dovish} {FinTech Lab GitHub}.}.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts"
      ],
      "metadata": {
        "id": "dVLspkBLV9rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding section names and basic prompt instructions to each prompt\n",
        "\n",
        "section_names = []\n",
        "for node in base_nodes:\n",
        "    section_names.append(node.id_)\n",
        "\n",
        "# Join the node names with commas and the last one with 'and', all enclosed in single quotes\n",
        "quoted_names = [f\"'{name}'\" for name in section_names]\n",
        "section_names_text = ', '.join(quoted_names[:-1]) + ', and ' + quoted_names[-1]\n",
        "\n",
        "# Old Instruction\n",
        "#prompt_instruction = f\"\"\"If the the answer is 'YES', provide the section name.\n",
        "#The section names are {section_names_text}.\n",
        "#If the answer is 'NO' or 'NOT APPLICABLE', provide a justification.\n",
        "#Provide the answer in the first line and provide the section name or justification in the second line.\"\"\"\n",
        "\n",
        "# NEED TO HAVE A SEPARATE PROMPT INSTRUCTION FOR A3\n",
        "prompt_instruction = f\"\"\"If the the answer is 'YES', provide the section name.\n",
        "The only valid section names are {section_names_text}.\n",
        "If the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'.\n",
        "Provide a step by step justification for the answer.\n",
        "Format your response as a JSON object with 'answer', 'section name', and 'justification' as the keys.\n",
        "If the information isn't present, use 'unknown' as the value.\"\"\""
      ],
      "metadata": {
        "id": "7BSTyyENTgyH"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supporting_prompt_dict = OrderedDict()\n",
        "\n",
        "supporting_prompt_dict[\"A1\"] = \"\"\"Point out any strong assumptions and how robust your results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only held locally). Reflect on how these assumptions might be violated in practice and what the implications would be.\n",
        "Reflect on the scope of your claims, e.g., if you only tested your approach on a few datasets, languages, or did a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. Reflect on the factors that influence the performance of your approach. For example, a speech-to-text system might not be able to be reliably used to provide closed captions for online lectures because it fails to handle technical jargon.\n",
        "If you analyze model biases: state the definition of bias you are using. State the motivation and definition explicitly.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"A2\"] = \"\"\"Examples of risks include potential malicious or unintended harmful effects and uses (e.g., disinformation, generating fake profiles, surveillance), environmental impact (e.g., training huge models), fairness considerations (e.g., deployment of technologies that could further disadvantage or exclude historically disadvantaged groups), privacy considerations (e.g., a paper on model/data stealing), and security considerations (e.g., adversarial attacks).\n",
        "Consider if the research contributes to overgeneralization, bias confirmation, under or overexposure of specific languages, topics, or applications at the expense of others.\n",
        "We expect many papers to be foundational research and not tied to particular applications, let alone deployments. However, we encourage authors to discuss potential risks if they see a path to any positive or negative applications. For example, the authors can emphasize how their systems are intended to be used, how they can safeguard their systems against misuse, or propose future research directions.\n",
        "Consider different stakeholders that could be impacted by your work. Consider if it possible that research benefits some stakeholders while harming others. Consider if it pays special attention to vulnerable or marginalized communities. Consider if the research leads to exclusion of certain groups.\n",
        "Consider dual use, i.e, possible benefits or harms that could arise when the technology is being used as intended and functioning correctly, benefits or harms that could arise when the technology is being used as intended but gives incorrect results, and benefits or harms following from (intentional or unintentional) misuse of the technology.\n",
        "Consider citing previous work on relevant mitigation strategies for the potential risks of the work (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of NLP).\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"A3\"] = \"\"\"The main claims in the paper should be clearly stated in the abstract and in the introduction.\n",
        "These claims should be supported by evidence presented in the paper, potentially in the form of experimental results, reasoning, or theory. The connection between which evidence supports which claims should be clear.\n",
        "The context of the contributions of the paper should be clearly described, and it should be stated how much the results would be expected to generalize to other contexts.\n",
        "It should be easy for a casual reader to distinguish between the contributions of the paper and open questions, future work, aspirational goals, motivations, etc.\"\"\"\n",
        "\n",
        "#supporting_prompt_dict[\"B1\"] = \"\"\"For composite artifacts like the GLUE benchmark, this means all creators. Cite the original paper that produced the code package or dataset. Remember to state which version of the asset you’re using.\n",
        "#Sections with URLs are more likely to have scientific artifacts.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"B1\"] = \"\"\"For composite artifacts like the GLUE benchmark, this means all creators. Cite the original paper that produced the code package or dataset. Remember to state which version of the asset you’re using.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"B2\"] = \"\"\"State the name of the license (e.g., CC-BY 4.0) for each asset.\n",
        "If you scraped or collected data from a particular source (e.g., website or social media API), you should state the copyright and terms of service of that source.\n",
        "Please note that some sources do not allow inference of protected categories like gender, sexual orientation, health status, etc. The data might be in public domain and licensed for research purposes. The data might be used with consent of its creators or copyright holders.\n",
        "If the data is used without consent, the paper makes the case to justify its legal basis (e.g., research performed in the public interest under GDPR).\n",
        "If you are releasing assets, you should include a license, copyright information, and terms of use in the package.\n",
        "If you are repackaging an existing dataset, you should state the original license as well as the one for the derived asset (if it has changed).\n",
        "If you cannot find this information online, you are encouraged to reach out to the asset’s creators.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"B3\"] = \"\"\"For the artifacts you create, specify the intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts).\n",
        "Data and/or pretrained models are released under a specified license that is compatible with the conditions under which access to data was granted (in particular, derivatives of data accessed for research purposes should not be deployed in the real world as anything other than a research prototype, especially commercially).\n",
        "The paper specifies the efforts to limit the potential use to circumstances in which the data/models could be used safely (such as an accompanying data/model statement).\n",
        "The data is sufficiently anonymized to make identification of individuals impossible without significant effort. If this is not possible due to the research type, please state so explicitly and explain why.\n",
        "The paper discusses the harms that may ensue from the limitations of the data collection methodology, especially concerning marginalized/vulnerable populations, and specifies the scope within which the data can be used safely.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"B4\"] = \"\"\"There are some settings where the existence of offensive content is not necessarily bad (e.g., swear words occur naturally in text), or part of the research question (i.e., hate speech). This question is just to encourage discussion of potentially undesirable properties.\n",
        "Explain how you checked for offensive content and identifiers (e.g., with a script, manually on a sample, etc.).\n",
        "Explain how you anonymized the data, i.e., removed identifying information like names, phone and credit card numbers, addresses, user names, etc. Examples are monodirectional hashes, replacement, or removal of data points. If anonymization is not possible due to the nature of the research (e.g., author identification), explain why.\n",
        "List any further privacy protection measures you are using: separation of author metadata from text, licensing, etc.\n",
        "If any personal data is used: the paper specifies the standards applied for its storage and processing, and any anonymization efforts.\n",
        "If the individual speakers remain identifiable via search: the paper discusses possible harms from misuse of this data, and their mitigation.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"B5\"] = \"\"\"Scientific artifacts may include code, data, models or other artifacts. Be sure to report the language of any language data, even if it is commonly-used benchmarks.\n",
        "Describe basic information about the data that was used, such as the domain of the text, any information about the demographics of the authors, etc.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"B6\"] = \"\"\"Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"C1\"] = \"\"\"Even for commonly-used models like BERT, reporting the number of parameters is important because it provides context necessary for readers to understand experimental results. The size of a model has an impact on performance, and it shouldn’t be up to a reader to have to go look up the number of parameters in models to remind themselves of this information.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"C2\"] = \"\"\"The experimental setup should include information about exactly how experiments were set up, like how model selection was done (e.g., early stopping on validation data, the single model with the lowest loss, etc.), how data was preprocessed, etc.\n",
        "Many research projects involve manually tuning hyperparameters until some “good” values are found, and then running a final experiment which is reported in the paper. Other projects involve using random search or grid search to find hyperparameters. In all cases, report the results of such experiments, even if they were stopped early or didn’t lead to your best results, as it allows a reader to know the process necessary to get to the final result and to estimate which hyperparameters were important to tune.\n",
        "Be sure to include the best-found hyperparameter values (e.g., learning rate, regularization, etc.) as these are critically important for others to build on your work.\n",
        "The experimental setup should likely be described in the main body of the paper, as that is important for reviewers to understand the results, but large tables of hyperparameters or the results of hyperparameter searches could be presented in the main paper or appendix.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"C3\"] = \"\"\"Error bars can be computed by running experiments with different random seeds, Clopper–Pearson confidence intervals can be placed around the results (e.g., accuracy), or expected validation performance can be useful tools here.\n",
        "In all cases, when a result is reported, it should be clear if it is from a single run, the max across N random seeds, the average, etc.\n",
        "When reporting a result on a test set, be sure to report a result of the same model on the validation set (if available) so others reproducing your work don’t need to evaluate on the test set to confirm a reproduction.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"C4\"] = \"\"\"The version number or reference to specific implementation is important because different implementations of the same metric can lead to slightly different results (e.g., ROUGE).\n",
        "The paper cites the original work for the model or software package. If no paper exists, a URL to the website or repository is included.\n",
        "If you modified an existing library, explain what changes you made.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"D1\"] = \"\"\"Examples of risks include a crowdsourcing experiment which might show offensive content or collect personal identifying information (PII). Ideally, the participants should be warned.\n",
        "Including this information in the supplemental material is fine, but if the main contribution of your paper involves human subjects, then we strongly encourage you to include as much detail as possible in the main paper.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"D2\"] = \"\"\"Be explicit about how you recruited your participants. For instance, mention the specific crowdsourcing platform used. If participants are students, give information about the population (e.g., graduate/undergraduate, from a specific field), and how they were compensated (e.g., for course credit or through payment).\n",
        "In case of payment, provide the amount paid for each task (including any bonuses), and discuss how you determined the amount of time a task would take. Include discussion on how the wage was determined and how you determined that this was a fair wage.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"D3\"] = \"\"\"For example, if the was collect via crowdsourcing, the instructions should explain to crowdworkers how the data would be used.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"D4\"] = \"\"\"Depending on the country in which research is conducted, ethics review (e.g., from an IRB board in the US context) may be required for any human subjects research. If an ethics review board was involved, you should clearly state it in the paper. However, stating that you obtained approval from an ethics review board does not imply that the societal impact of the work does not need to be discussed.\n",
        "For initial submissions, do not include any information that would break anonymity, such as the institution conducting the review.\"\"\"\n",
        "\n",
        "supporting_prompt_dict[\"D5\"] = \"\"\"State if your data include any protected information (e.g., sexual orientation or political views under GDPR).\n",
        "The paper is accompanied by a data statement describing the basic demographic and geographic characteristics of the author population that is the source of the data, and the population that it is intended to represent.\n",
        "If applicable: the paper describes whether any characteristics of the human subjects were self-reported (preferably) or inferred (in what way), justifying the methodology and choice of description categories.\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "fBPH5XEmhm1v"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://aclrollingreview.org/responsibleNLPresearch/\n",
        "\n",
        "prompt_dict = OrderedDict()\n",
        "\n",
        "## A for Every Submission\n",
        "###\n",
        "prompt_dict[\"A1\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you describe the limitations of your work?\n",
        "Additional Context: {supporting_prompt_dict[\"A1\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"A2\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you discuss any potential risks of your work?\n",
        "Additional Context: {supporting_prompt_dict[\"A2\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"A3\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Do the abstract and introduction summarize the paper’s main claims?\n",
        "Additional Context: {supporting_prompt_dict[\"A3\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "## B Did you use or create scientific artifacts?\n",
        "###\n",
        "prompt_dict[\"B1\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference. Scientific artifacts may include code, data, models or other artifacts.\n",
        "Question: Did you cite the creators of artifacts you used?\n",
        "Additional Context: {supporting_prompt_dict[\"B1\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"B2\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference. Scientific artifacts may include code, data, models or other artifacts.\n",
        "Question: Did you discuss the license or terms for use and/or distribution of any scientific artifacts?\n",
        "Additional Context: {supporting_prompt_dict[\"B2\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"B3\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference. Scientific artifacts may include code, data, models or other artifacts.\n",
        "Question: Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified?\n",
        "Additional Context: {supporting_prompt_dict[\"B3\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"B4\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\n",
        "Additional Context: {supporting_prompt_dict[\"B4\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"B5\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Scientific artifacts may include code, data, models or other artifacts. Question: Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\n",
        "Additional Context: {supporting_prompt_dict[\"B5\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"B6\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created?\n",
        "Additional Context: {supporting_prompt_dict[\"B6\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "## C Did you run computational experiments\n",
        "###\n",
        "prompt_dict[\"C1\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), or computing infrastructure used?\n",
        "Additional Context: {supporting_prompt_dict[\"C1\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"C2\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\n",
        "Additional Context: {supporting_prompt_dict[\"C2\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"C3\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\n",
        "Additional Context: {supporting_prompt_dict[\"C3\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"C4\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\n",
        "Additional Context: {supporting_prompt_dict[\"C4\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "## D Did you use human annotators (e.g., crowdworkers) or research with human participants?\n",
        "###\n",
        "\n",
        "prompt_dict[\"D1\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\n",
        "Additional Context: {supporting_prompt_dict[\"D1\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"D2\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants’ demographic (e.g., country of residence)?\n",
        "Additional Context: {supporting_prompt_dict[\"D2\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"D3\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you discuss whether and how consent was obtained from people whose data you’re using/curating?\n",
        "Additional Context: {supporting_prompt_dict[\"D3\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"D4\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Was the data collection protocol approved (or determined exempt) by an ethics review board?\n",
        "Additional Context: {supporting_prompt_dict[\"D4\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "prompt_dict[\"D5\"] = f\"\"\"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
        "Question: Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\n",
        "Additional Context: {supporting_prompt_dict[\"D5\"]}\n",
        "Output Structure: \"\"\" + prompt_instruction\n",
        "\n",
        "## E Did you use AI assistants (e.g., ChatGPT, Copilot) in your research, coding, or writing?\n",
        "###\n",
        "\n",
        "# E1. Did you include information about your use of AI assistants?\n",
        "\n",
        "# E1. Elaboration For Yes Or No. For yes, provide a section number. For no, justify why not.\n",
        "\n",
        "# E1. Section Or Justification\n"
      ],
      "metadata": {
        "id": "xSgfT0PyTg4R"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_dict[\"D3\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN6_QtNsVROR",
        "outputId": "1ee7b816-3d6a-412a-bd44-b2e5b1a41a04"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
            "Question: Did you discuss whether and how consent was obtained from people whose data you’re using/curating?\n",
            "Additional Context: For example, if the was collect via crowdsourcing, the instructions should explain to crowdworkers how the data would be used.\n",
            "Output Structure: If the the answer is 'YES', provide the section name. \n",
            "The only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \n",
            "If the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \n",
            "Provide a step by step justification for the answer.\n",
            "Format your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \n",
            "If the information isn't present, use 'unknown' as the value.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select Embedding Model and LLM"
      ],
      "metadata": {
        "id": "PaGeFgQIBch-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Local embedding model\n",
        "#embed_model = resolve_embed_model(\"local:BAAI/bge-large-en-v1.5\")\n",
        "\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "# Used to output json files.\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "llm = OpenAI(model=model_name, chunk_size_limit=2048)\n",
        "\n",
        "# Testing for LLMRerank with bad documentation\n",
        "#Settings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "#Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "flVs3JavENWw"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk References: Smaller Child Chunks Referring to Bigger Parent Chunk\n",
        "\n",
        "In this usage example, we show how to build a graph of smaller chunks pointing to bigger parent chunks.\n",
        "\n",
        "During query-time, we retrieve smaller chunks, but we follow references to bigger chunks. This allows us to have more context for synthesis."
      ],
      "metadata": {
        "id": "tmjJEUx8b9h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# semantic chunking\n",
        "# https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/\n",
        "# Best source: https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
        "# Not llama index, but good idea: https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/\n",
        "\n",
        "sub_node_parsers = [SemanticSplitterNodeParser(buffer_size=1,\n",
        "                                               breakpoint_percentile_threshold=90,\n",
        "                                               embed_model=embed_model,\n",
        "                                               include_metadata = True,\n",
        "                                               include_prev_next_rel = True),]\n",
        "\n",
        "# chunk_overlap is something to tune\n",
        "#sub_chunk_sizes = [512]\n",
        "#sub_node_parsers = [SentenceSplitter(chunk_size=c, chunk_overlap = 400) for c in sub_chunk_sizes]"
      ],
      "metadata": {
        "id": "nuPodorPXDwO"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_nodes = []\n",
        "\n",
        "# To understand this well check out Sophia's Tutorial: https://youtu.be/ihSiRrOUwmg?si=Q5wVyVBkAovZBGHi&t=45 (images only in video version)\n",
        "# https://towardsdatascience.com/advanced-rag-01-small-to-big-retrieval-172181b396d4\n",
        "for base_node in base_nodes:\n",
        "    for n in sub_node_parsers:\n",
        "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
        "        sub_inodes = [\n",
        "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
        "        ]\n",
        "        all_nodes.extend(sub_inodes)\n",
        "\n",
        "    # also add original node to node\n",
        "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
        "    all_nodes.append(original_node)\n",
        "\n",
        "all_nodes_dict = {n.node_id: n for n in all_nodes}"
      ],
      "metadata": {
        "id": "sXTNcc-2b9n3"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex(all_nodes, embed_model=embed_model)"
      ],
      "metadata": {
        "id": "xUtQ8CG997fe"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for node in all_nodes:\n",
        "    print(node.id_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY4OIPbVORok",
        "outputId": "3ba7de8a-df94-4219-f2d2-f415d7fec833"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7b7331d1-6cce-48a2-bb56-ce0b871caf4f\n",
            "77b95f69-efef-4dfd-a674-8552c41d82d0\n",
            "abstract\n",
            "3f99c9ef-86b8-437b-aac0-5b76c4d8f4ff\n",
            "8f1e098f-410d-4bbf-920d-8c58972e58d5\n",
            "216f2265-dd1a-4793-b35d-10b2bd8c1062\n",
            "b31e4998-9bef-4818-8118-0c60fab7b67b\n",
            "1 Introduction\n",
            "b62c8f52-5b6b-4686-bf7a-65eb4e80a7e9\n",
            "d239c82d-07ca-46c3-b36b-0d96c0704b55\n",
            "0a61a5ef-b0da-4e57-9a20-10aafe547c02\n",
            "2 Related Work\n",
            "2f8d2342-909c-4b5c-a1ca-279d19ee15ef\n",
            "5960599f-c517-419a-b215-7a6049723cdc\n",
            "a5f94897-3016-42c1-a333-c18676d8739d\n",
            "de3a520d-1112-403d-a920-883ac6b5d295\n",
            "f18da1b8-0515-402b-9ff4-688a150eb08b\n",
            "b92c933d-2b33-49e1-b803-60c30022b603\n",
            "2e1c5483-2e4b-4a09-9e87-c0bbad65bc56\n",
            "7fe4a84e-4b5d-4d35-849a-c95c6f67d14c\n",
            "3 Dataset\n",
            "adf7bcd3-b6cd-4917-835f-f81261449e1d\n",
            "1a978bb3-f00e-4dd6-95a6-90db64f94bf2\n",
            "85d411c3-3c1f-4600-92f6-82abf4147ddc\n",
            "218b07a2-5105-4dc4-b5ef-08a30fc54494\n",
            "77b1c13e-7862-48d7-8503-174f0cd2623c\n",
            "210af792-d801-47d6-8c79-a335549b76dd\n",
            "86a9578c-938b-475a-8456-966d0ed8fbc0\n",
            "4 Models\n",
            "c9ac401b-2c06-4428-acbf-3347911c88e8\n",
            "9c101c60-37c2-4de0-86fa-ae22e0ebcccf\n",
            "1d183540-1f98-474e-90d8-93fc33f01a29\n",
            "5478aca2-e4d3-4220-9cd6-f04cc310024a\n",
            "0ebc205e-f8a5-4fe8-b1ca-cab6d2977e11\n",
            "180037d1-a7a3-4e03-838f-908fee475d79\n",
            "e74a8f62-05ba-4b9a-82ab-59bcef3369c9\n",
            "e64838f5-aa0b-4ae6-87d1-7b4af62800f4\n",
            "5 Results and Analysis\n",
            "addb0593-ec6a-45b5-a6d9-4574c10344fc\n",
            "11b62c9a-b3f9-4163-8d47-81272e67d3b1\n",
            "6 Conclusion\n",
            "665b1b6e-1fed-43bc-a04f-19d5e5249437\n",
            "cc2be176-49ee-49d4-918a-25ca2becdc3b\n",
            "Limitations\n",
            "1b9c8ef6-ee75-40d1-a6cd-97da06d655b8\n",
            "a0085e2b-1f3d-4f6d-a461-90b2fb4ae8bd\n",
            "Ethics Statement\n",
            "e31919ae-cb57-41f1-96f6-5d46fd89490a\n",
            "51fcf185-45a6-41c6-8463-20168cebe4b7\n",
            "A FinBERT Sentiment Analysis\n",
            "fe96970c-c427-4e07-8fd5-a4988f354ab5\n",
            "661a9ed4-c76c-4a2c-b482-28080a6838c5\n",
            "B Transfer Learning\n",
            "d42438b7-0884-49d2-9f36-6a217f2097a7\n",
            "48ccbf08-fa53-41c4-8383-a3d99911cbab\n",
            "de342ae6-0f09-433f-9b63-ddeaaa84a975\n",
            "C Manual Annotation\n",
            "9ecc81d7-2cdd-411d-b47e-96de81c61959\n",
            "1e37e3b3-7e19-43b7-b1dc-edca239272ee\n",
            "D Robustness check\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying using LLM YES NO N/A\n",
        "\n",
        "Going to add reranker again and node_postprocessors in next notebook."
      ],
      "metadata": {
        "id": "_c07QVO_9xAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is something that should be changed over time (we can add meta )\n",
        "vector_retriever_chunk = index.as_retriever(similarity_top_k=40)\n",
        "\n",
        "recursive_retriever = RecursiveRetriever(\n",
        "    \"vector\",\n",
        "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
        "    node_dict=all_nodes_dict,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "# https://docs.llamaindex.ai/en/v0.10.17/module_guides/deploying/query_engine/response_modes.html\n",
        "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    recursive_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        "    llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "PQU2VdDCulAO"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_dict['A1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9ofxP9PvLr7",
        "outputId": "99e82512-0443-4463-96e1-724e372a3f35"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduction: Behave like you are the author of a paper you are going to submit to a conference.\n",
            "Question: Did you describe the limitations of your work?\n",
            "Additional Context: Point out any strong assumptions and how robust your results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only held locally). Reflect on how these assumptions might be violated in practice and what the implications would be.\n",
            "Reflect on the scope of your claims, e.g., if you only tested your approach on a few datasets, languages, or did a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. Reflect on the factors that influence the performance of your approach. For example, a speech-to-text system might not be able to be reliably used to provide closed captions for online lectures because it fails to handle technical jargon.\n",
            "If you analyze model biases: state the definition of bias you are using. State the motivation and definition explicitly.\n",
            "Output Structure: If the the answer is 'YES', provide the section name. \n",
            "The only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \n",
            "If the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \n",
            "Provide a step by step justification for the answer.\n",
            "Format your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \n",
            "If the information isn't present, use 'unknown' as the value.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(query_engine.query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-29I36ICVjp",
        "outputId": "3ea186db-acac-4348-b98b-a665804efbbc"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method query in module llama_index.core.base.base_query_engine:\n",
            "\n",
            "query(str_or_query_bundle: Union[str, llama_index.core.schema.QueryBundle]) -> Union[llama_index.core.base.response.schema.Response, llama_index.core.base.response.schema.StreamingResponse, llama_index.core.base.response.schema.AsyncStreamingResponse, llama_index.core.base.response.schema.PydanticResponse] method of llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine instance\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwhuvgnuJNo_",
        "outputId": "bf954d6c-dcf4-4c21-ac03-cdd161b7f556"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'C1', 'C2', 'C3', 'C4', 'D1', 'D2', 'D3', 'D4', 'D5'])"
            ]
          },
          "metadata": {},
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(prompt_dict['D5'])"
      ],
      "metadata": {
        "id": "SjA-9-rpz48h"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKyobOkC3QSe",
        "outputId": "728b091a-1d4e-44ae-cb0d-6633635fd84c"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answer\": \"YES\",\n",
            "  \"section name\": \"C Manual Annotation\",\n",
            "  \"justification\": \"The basic demographic and geographic characteristics of the annotator population are described in the section 'C Manual Annotation'.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outputting JSON Response."
      ],
      "metadata": {
        "id": "VFnmpLUyMN4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "for index ,key in enumerate(prompt_dict.keys()):\n",
        "    response = query_engine.query(prompt_dict[key])\n",
        "    temp_dict = json.loads(response.response)\n",
        "    temp_dict['prompt'] = prompt_dict[key]\n",
        "    temp_dict['llm'] = model_name\n",
        "\n",
        "    results[key] = temp_dict"
      ],
      "metadata": {
        "id": "79sK6rmIvJXP"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to JSON file\n",
        "with open('sample_output.json', 'w') as file:\n",
        "    json.dump(results, file)"
      ],
      "metadata": {
        "id": "7x9rnGhuRj9S"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just code to check the thing i output (Read and parse the JSON file)\n",
        "filename = 'sample_output.json'  # Gets the name of the first uploaded file\n",
        "with open(filename, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Display the content\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrppNM6DR90a",
        "outputId": "c45f2a9f-da53-4fde-d6f2-56c9a4a477cc"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A1': {'answer': 'YES', 'section name': 'Limitations', 'justification': 'The introduction mentions the limitations of the traditional rule-based approach in measuring monetary policy stance, indicating an awareness of the limitations of existing methods. It also highlights the need for a new dataset and task for hawkish-dovish classification, suggesting an understanding of the limitations of current sentiment analysis models in capturing policy stance.', 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you describe the limitations of your work?\\nAdditional Context: Point out any strong assumptions and how robust your results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only held locally). Reflect on how these assumptions might be violated in practice and what the implications would be.\\nReflect on the scope of your claims, e.g., if you only tested your approach on a few datasets, languages, or did a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. Reflect on the factors that influence the performance of your approach. For example, a speech-to-text system might not be able to be reliably used to provide closed captions for online lectures because it fails to handle technical jargon.\\nIf you analyze model biases: state the definition of bias you are using. State the motivation and definition explicitly.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'A2': {'answer': 'YES', 'section name': 'Ethics Statement', 'justification': 'The authors acknowledge the geographic bias in the study and the gender bias in the observation period. They mention the limitation of the study to the Federal Reserve Bank of the United States of America and the lack of gender diversity in the Fed chairs. They also mention the ethical use of publicly available data and the consideration of the license category for using the language models.', 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you discuss any potential risks of your work?\\nAdditional Context: Examples of risks include potential malicious or unintended harmful effects and uses (e.g., disinformation, generating fake profiles, surveillance), environmental impact (e.g., training huge models), fairness considerations (e.g., deployment of technologies that could further disadvantage or exclude historically disadvantaged groups), privacy considerations (e.g., a paper on model/data stealing), and security considerations (e.g., adversarial attacks).\\nConsider if the research contributes to overgeneralization, bias confirmation, under or overexposure of specific languages, topics, or applications at the expense of others.\\nWe expect many papers to be foundational research and not tied to particular applications, let alone deployments. However, we encourage authors to discuss potential risks if they see a path to any positive or negative applications. For example, the authors can emphasize how their systems are intended to be used, how they can safeguard their systems against misuse, or propose future research directions.\\nConsider different stakeholders that could be impacted by your work. Consider if it possible that research benefits some stakeholders while harming others. Consider if it pays special attention to vulnerable or marginalized communities. Consider if the research leads to exclusion of certain groups.\\nConsider dual use, i.e, possible benefits or harms that could arise when the technology is being used as intended and functioning correctly, benefits or harms that could arise when the technology is being used as intended but gives incorrect results, and benefits or harms following from (intentional or unintentional) misuse of the technology.\\nConsider citing previous work on relevant mitigation strategies for the potential risks of the work (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of NLP).\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'A3': {'answer': 'YES', 'section name': 'abstract', 'justification': \"The abstract clearly summarizes the main claims of the paper, including the construction of a novel dataset, development of a hawkish-dovish classification task, benchmarking of language models, construction of a monetary policy stance measure, and evaluation of the measure's impact on financial markets.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Do the abstract and introduction summarize the paper’s main claims?\\nAdditional Context: The main claims in the paper should be clearly stated in the abstract and in the introduction.\\nThese claims should be supported by evidence presented in the paper, potentially in the form of experimental results, reasoning, or theory. The connection between which evidence supports which claims should be clear.\\nThe context of the contributions of the paper should be clearly described, and it should be stated how much the results would be expected to generalize to other contexts.\\nIt should be easy for a casual reader to distinguish between the contributions of the paper and open questions, future work, aspirational goals, motivations, etc.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'B1': {'answer': 'YES', 'section name': '1 Introduction', 'justification': 'The introduction section mentions the need to develop a new task for hawkish vs dovish classification and test the performance of various models, including rule-based and fine-tuned large PLMs. It also discusses the contributions made by the authors in terms of introducing a new task, building datasets, and developing an aggregate monetary policy stance measure.', 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference. Scientific artifacts may include code, data, models or other artifacts.\\nQuestion: Did you cite the creators of artifacts you used?\\nAdditional Context: For composite artifacts like the GLUE benchmark, this means all creators. Cite the original paper that produced the code package or dataset. Remember to state which version of the asset you’re using.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'B2': {'answer': 'YES', 'section name': 'abstract', 'justification': 'The license information for the scientific artifacts, including the dataset, models, and code, is discussed in the abstract section. The license mentioned is CC BY-NC 4.0.', 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference. Scientific artifacts may include code, data, models or other artifacts.\\nQuestion: Did you discuss the license or terms for use and/or distribution of any scientific artifacts?\\nAdditional Context: State the name of the license (e.g., CC-BY 4.0) for each asset.\\nIf you scraped or collected data from a particular source (e.g., website or social media API), you should state the copyright and terms of service of that source.\\nPlease note that some sources do not allow inference of protected categories like gender, sexual orientation, health status, etc. The data might be in public domain and licensed for research purposes. The data might be used with consent of its creators or copyright holders.\\nIf the data is used without consent, the paper makes the case to justify its legal basis (e.g., research performed in the public interest under GDPR).\\nIf you are releasing assets, you should include a license, copyright information, and terms of use in the package.\\nIf you are repackaging an existing dataset, you should state the original license as well as the one for the derived asset (if it has changed).\\nIf you cannot find this information online, you are encouraged to reach out to the asset’s creators.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'B3': {'answer': 'YES', 'section name': 'abstract', 'justification': 'The paper specifies that the data and/or pretrained models are released under a specified license that is compatible with the conditions under which access to data was granted, ensuring that derivatives of data accessed for research purposes are not used outside of research contexts. The artifacts are intended for research purposes and are released under a license that aligns with this intended use.', 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference. Scientific artifacts may include code, data, models or other artifacts.\\nQuestion: Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified?\\nAdditional Context: For the artifacts you create, specify the intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts).\\nData and/or pretrained models are released under a specified license that is compatible with the conditions under which access to data was granted (in particular, derivatives of data accessed for research purposes should not be deployed in the real world as anything other than a research prototype, especially commercially).\\nThe paper specifies the efforts to limit the potential use to circumstances in which the data/models could be used safely (such as an accompanying data/model statement).\\nThe data is sufficiently anonymized to make identification of individuals impossible without significant effort. If this is not possible due to the research type, please state so explicitly and explain why.\\nThe paper discusses the harms that may ensue from the limitations of the data collection methodology, especially concerning marginalized/vulnerable populations, and specifies the scope within which the data can be used safely.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'B4': {'answer': 'YES', 'section name': '3 Dataset', 'justification': \"The dataset creation process involved steps to filter out irrelevant sentences and isolate key sentences relevant to changes in the Federal Reserve's monetary policy stance. Offensive content and identifiers were likely checked during this process to ensure the dataset's quality and appropriateness for analysis.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\nAdditional Context: There are some settings where the existence of offensive content is not necessarily bad (e.g., swear words occur naturally in text), or part of the research question (i.e., hate speech). This question is just to encourage discussion of potentially undesirable properties.\\nExplain how you checked for offensive content and identifiers (e.g., with a script, manually on a sample, etc.).\\nExplain how you anonymized the data, i.e., removed identifying information like names, phone and credit card numbers, addresses, user names, etc. Examples are monodirectional hashes, replacement, or removal of data points. If anonymization is not possible due to the nature of the research (e.g., author identification), explain why.\\nList any further privacy protection measures you are using: separation of author metadata from text, licensing, etc.\\nIf any personal data is used: the paper specifies the standards applied for its storage and processing, and any anonymization efforts.\\nIf the individual speakers remain identifiable via search: the paper discusses possible harms from misuse of this data, and their mitigation.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'B5': {'answer': 'YES', 'section name': '3 Dataset', 'justification': \"The dataset used in the study is described in detail in the '3 Dataset' section, including information about the domain of the text (FOMC speeches, meeting minutes, press conference transcripts), the time period covered (1996-2022), data collection methods, and data preprocessing steps.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nScientific artifacts may include code, data, models or other artifacts. Question: Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\nAdditional Context: Scientific artifacts may include code, data, models or other artifacts. Be sure to report the language of any language data, even if it is commonly-used benchmarks.\\nDescribe basic information about the data that was used, such as the domain of the text, any information about the demographics of the authors, etc.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'B6': {'answer': 'YES', 'section name': '3 Dataset', 'justification': \"The section '3 Dataset' provides relevant statistics such as the number of examples in the labeled dataset before and after splitting, details on the sampling procedure, and information on the annotation agreement.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nDid you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created?\\nAdditional Context: Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'C1': {'answer': 'YES', 'section name': '4 Models', 'justification': \"The section '4 Models' provides detailed information about the models used, including the number of parameters, training-validation split, vocabulary size, and the computing infrastructure (NVIDIA RTX A6000 GPU) used for implementation.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), or computing infrastructure used?\\nAdditional Context: Even for commonly-used models like BERT, reporting the number of parameters is important because it provides context necessary for readers to understand experimental results. The size of a model has an impact on performance, and it shouldn’t be up to a reader to have to go look up the number of parameters in models to remind themselves of this information.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'C2': {'answer': 'YES', 'section name': '4 Models', 'justification': \"The experimental setup, including hyperparameter search and best-found hyperparameter values, is discussed in the '4 Models' section where it mentions performing a grid search on different learning rates and batch sizes to find the best hyperparameters for each model.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\nAdditional Context: The experimental setup should include information about exactly how experiments were set up, like how model selection was done (e.g., early stopping on validation data, the single model with the lowest loss, etc.), how data was preprocessed, etc.\\nMany research projects involve manually tuning hyperparameters until some “good” values are found, and then running a final experiment which is reported in the paper. Other projects involve using random search or grid search to find hyperparameters. In all cases, report the results of such experiments, even if they were stopped early or didn’t lead to your best results, as it allows a reader to know the process necessary to get to the final result and to estimate which hyperparameters were important to tune.\\nBe sure to include the best-found hyperparameter values (e.g., learning rate, regularization, etc.) as these are critically important for others to build on your work.\\nThe experimental setup should likely be described in the main body of the paper, as that is important for reviewers to understand the results, but large tables of hyperparameters or the results of hyperparameter searches could be presented in the main paper or appendix.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'C3': {'answer': 'YES', 'section name': '5 Results and Analysis', 'justification': \"The section '5 Results and Analysis' discusses the performance of different NLP models on the hawkish vs dovish classification task, including the best-performing model, validation data, and the need for a robustness check to ensure no look-ahead bias. It provides detailed insights into the model performance and the methodology used to evaluate the results.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\nAdditional Context: Error bars can be computed by running experiments with different random seeds, Clopper–Pearson confidence intervals can be placed around the results (e.g., accuracy), or expected validation performance can be useful tools here.\\nIn all cases, when a result is reported, it should be clear if it is from a single run, the max across N random seeds, the average, etc.\\nWhen reporting a result on a test set, be sure to report a result of the same model on the validation set (if available) so others reproducing your work don’t need to evaluate on the test set to confirm a reproduction.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'C4': {'answer': 'YES', 'section name': '4 Models', 'justification': \"The section '4 Models' discusses the implementation, model, and parameter settings used for various models including LSTM, Bi-LSTM, PLMs, and ChatGPT. It mentions the use of PyTorch, TensorFlow, and Huggingface Transformers library for implementation, along with details on hyperparameters and model configurations.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\nAdditional Context: The version number or reference to specific implementation is important because different implementations of the same metric can lead to slightly different results (e.g., ROUGE).\\nThe paper cites the original work for the model or software package. If no paper exists, a URL to the website or repository is included.\\nIf you modified an existing library, explain what changes you made.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'D1': {'answer': 'NO', 'section name': 'None', 'justification': 'The paper does not report the full text of instructions given to participants, including any disclaimers of risks to participants or annotators. The paper focuses on constructing a dataset, developing a novel task, benchmarking models, and evaluating the impact on financial markets, without involving human subjects or crowdsourcing experiments that would require such detailed instructions.', 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\nAdditional Context: Examples of risks include a crowdsourcing experiment which might show offensive content or collect personal identifying information (PII). Ideally, the participants should be warned.\\nIncluding this information in the supplemental material is fine, but if the main contribution of your paper involves human subjects, then we strongly encourage you to include as much detail as possible in the main paper.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'D2': {'answer': 'YES', 'section name': '3 Dataset', 'justification': \"The section '3 Dataset' provides explicit information on how participants were recruited, including the use of specific tools like BeautifulSoup and Selenium for data collection. It also discusses the compensation for participants and how the data was obtained.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants’ demographic (e.g., country of residence)?\\nAdditional Context: Be explicit about how you recruited your participants. For instance, mention the specific crowdsourcing platform used. If participants are students, give information about the population (e.g., graduate/undergraduate, from a specific field), and how they were compensated (e.g., for course credit or through payment).\\nIn case of payment, provide the amount paid for each task (including any bonuses), and discuss how you determined the amount of time a task would take. Include discussion on how the wage was determined and how you determined that this was a fair wage.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'D3': {'answer': 'NO', 'section name': 'None', 'justification': 'The information regarding obtaining consent from people whose data is being used/curated is not discussed in any of the provided sections.', 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you discuss whether and how consent was obtained from people whose data you’re using/curating?\\nAdditional Context: For example, if the was collect via crowdsourcing, the instructions should explain to crowdworkers how the data would be used.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'D4': {'answer': 'YES', 'section name': 'Ethics Statement', 'justification': 'The Ethics Statement section explicitly mentions the acknowledgment of geographic and gender bias in the study, indicating an awareness of ethical considerations in the research.', 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Was the data collection protocol approved (or determined exempt) by an ethics review board?\\nAdditional Context: Depending on the country in which research is conducted, ethics review (e.g., from an IRB board in the US context) may be required for any human subjects research. If an ethics review board was involved, you should clearly state it in the paper. However, stating that you obtained approval from an ethics review board does not imply that the societal impact of the work does not need to be discussed.\\nFor initial submissions, do not include any information that would break anonymity, such as the institution conducting the review.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}, 'D5': {'answer': 'YES', 'section name': 'C Manual Annotation', 'justification': \"The section 'C Manual Annotation' provides detailed information about the annotators, including their gender, nationality, educational background, and expertise in finance-related coursework and macroeconomics.\", 'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\nAdditional Context: State if your data include any protected information (e.g., sexual orientation or political views under GDPR).\\nThe paper is accompanied by a data statement describing the basic demographic and geographic characteristics of the author population that is the source of the data, and the population that it is intended to represent.\\nIf applicable: the paper describes whether any characteristics of the human subjects were self-reported (preferably) or inferred (in what way), justifying the methodology and choice of description categories.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\", 'llm': 'gpt-3.5-turbo'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tKeQRBFR9jS",
        "outputId": "54e3bd9b-843c-4c2d-a65c-4f7a470d9507"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'C1', 'C2', 'C3', 'C4', 'D1', 'D2', 'D3', 'D4', 'D5'])"
            ]
          },
          "metadata": {},
          "execution_count": 335
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "SZjIYvzZvJbz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a81c11c-0ae7-4120-f4eb-9046b2f3d03b"
      },
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A1': {'answer': 'YES',\n",
              "  'section name': 'Limitations',\n",
              "  'justification': \"The limitations of the work are explicitly described in the 'Limitations' section, which includes aspects like dataset coverage, sentence splitting methodology, and the need for future research to address certain challenges.\",\n",
              "  'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you describe the limitations of your work?\\nAdditional Context: Point out any strong assumptions and how robust your results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only held locally). Reflect on how these assumptions might be violated in practice and what the implications would be.\\nReflect on the scope of your claims, e.g., if you only tested your approach on a few datasets, languages, or did a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. Reflect on the factors that influence the performance of your approach. For example, a speech-to-text system might not be able to be reliably used to provide closed captions for online lectures because it fails to handle technical jargon.\\nIf you analyze model biases: state the definition of bias you are using. State the motivation and definition explicitly.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\",\n",
              "  'llm': 'gpt-3.5-turbo'},\n",
              " 'A2': {'answer': 'YES',\n",
              "  'section name': 'Ethics Statement',\n",
              "  'justification': 'The author discusses potential risks related to geographic bias, gender bias, and the limitation of the study to the Federal Reserve Bank of the United States. The author also mentions the carbon footprint of pre-training large PLMs and the need to limit the work to fine-tuning existing PLMs.',\n",
              "  'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Did you discuss any potential risks of your work?\\nAdditional Context: Examples of risks include potential malicious or unintended harmful effects and uses (e.g., disinformation, generating fake profiles, surveillance), environmental impact (e.g., training huge models), fairness considerations (e.g., deployment of technologies that could further disadvantage or exclude historically disadvantaged groups), privacy considerations (e.g., a paper on model/data stealing), and security considerations (e.g., adversarial attacks).\\nConsider if the research contributes to overgeneralization, bias confirmation, under or overexposure of specific languages, topics, or applications at the expense of others.\\nWe expect many papers to be foundational research and not tied to particular applications, let alone deployments. However, we encourage authors to discuss potential risks if they see a path to any positive or negative applications. For example, the authors can emphasize how their systems are intended to be used, how they can safeguard their systems against misuse, or propose future research directions.\\nConsider different stakeholders that could be impacted by your work. Consider if it possible that research benefits some stakeholders while harming others. Consider if it pays special attention to vulnerable or marginalized communities. Consider if the research leads to exclusion of certain groups.\\nConsider dual use, i.e, possible benefits or harms that could arise when the technology is being used as intended and functioning correctly, benefits or harms that could arise when the technology is being used as intended but gives incorrect results, and benefits or harms following from (intentional or unintentional) misuse of the technology.\\nConsider citing previous work on relevant mitigation strategies for the potential risks of the work (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of NLP).\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\",\n",
              "  'llm': 'gpt-3.5-turbo'},\n",
              " 'A3': {'answer': 'YES',\n",
              "  'section name': 'abstract',\n",
              "  'justification': \"The abstract clearly summarizes the main claims of the paper, including the construction of a novel dataset, development of a hawkish-dovish classification task, benchmarking of language models, construction of a monetary policy stance measure, and evaluation of the measure's impact on financial markets. It also mentions the availability of the dataset, models, and code on Huggingface and GitHub.\",\n",
              "  'prompt': \"Introduction: Behave like you are the author of a paper you are going to submit to a conference.\\nQuestion: Do the abstract and introduction summarize the paper’s main claims?\\nAdditional Context: The main claims in the paper should be clearly stated in the abstract and in the introduction.\\nThese claims should be supported by evidence presented in the paper, potentially in the form of experimental results, reasoning, or theory. The connection between which evidence supports which claims should be clear.\\nThe context of the contributions of the paper should be clearly described, and it should be stated how much the results would be expected to generalize to other contexts.\\nIt should be easy for a casual reader to distinguish between the contributions of the paper and open questions, future work, aspirational goals, motivations, etc.\\nOutput Structure: If the the answer is 'YES', provide the section name. \\nThe only valid section names are 'abstract', '1 Introduction', '2 Related Work', '3 Dataset', '4 Models', '5 Results and Analysis', '6 Conclusion', 'Limitations', 'Ethics Statement', 'A FinBERT Sentiment Analysis', 'B Transfer Learning', 'C Manual Annotation', and 'D Robustness check'. \\nIf the answer is 'NO' or 'NOT APPLICABLE', the section name is 'None'. \\nProvide a step by step justification for the answer.\\nFormat your response as a JSON object with 'answer', 'section name', and 'justification' as the keys. \\nIf the information isn't present, use 'unknown' as the value.\",\n",
              "  'llm': 'gpt-3.5-turbo'}}"
            ]
          },
          "metadata": {},
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json.loads(response.response)"
      ],
      "metadata": {
        "id": "4ZxILALIvJgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cc07b8-545c-45d5-9670-1fe2058c766e"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'YES',\n",
              " 'section name': 'abstract',\n",
              " 'justification': \"The abstract clearly summarizes the main claims of the paper, including the construction of a novel dataset, development of a hawkish-dovish classification task, benchmarking of language models, construction of a monetary policy stance measure, and evaluation of the measure's impact on financial markets. It also mentions the availability of the dataset, models, and code on Huggingface and GitHub.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_string = \"\"\"\n",
        "{\n",
        "  \"answer\": \"YES\",\n",
        "  \"section name\": \"C Manual Annotation\",\n",
        "  \"justification\": \"The basic demographic and geographic characteristics of the annotator population are described in the section 'C Manual Annotation'.\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Convert the string to a dictionary\n",
        "data = json.loads(data_string)"
      ],
      "metadata": {
        "id": "itrqkjuLvJkM"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "hrkda7CBvJpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b0fb3d-deb6-4adc-871e-d6021d629c9d"
      },
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'YES',\n",
              " 'section name': 'C Manual Annotation',\n",
              " 'justification': \"The basic demographic and geographic characteristics of the annotator population are described in the section 'C Manual Annotation'.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [NOT USED AS OF 07/03] Currently works but want recursive retriever to make it more consistent."
      ],
      "metadata": {
        "id": "zOlBb9sgukgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes/\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=40,\n",
        "    node_postprocessors=[SafeLLMRerank(choice_batch_size=3, top_n=1),],\n",
        "    response_mode=\"refine\",\n",
        ")"
      ],
      "metadata": {
        "id": "A586wpQ3JuBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    response = query_engine.query(\n",
        "        prompt_dict['B1'],\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb6GRRI6l_vw",
        "outputId": "4e54b113-9b47-46d5-f893-771306eea6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rerank issue: invalid literal for int() with base 10: 'The question is asking about citing creators of artifacts used in a paper'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zAMfpL-cmW5M",
        "outputId": "290caa3a-ad7e-4922-b01a-1494a3cb0566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NO\\nJustification: The author did not mention citing the creators of artifacts used in the context provided.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 767
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Step Query Engine\n",
        "\n",
        "While the multi-step query engine didn't work well, the diagrams in the video are nice. Also it is possible that the multi steps will work well for longer prompts.\n",
        "\n",
        "Useful diagrams in this video that explains all of this:\n",
        "https://youtu.be/fdpaHJlN0PQ?si=5pqD-sD5zNHKzaOf&t=1242\n",
        "\n",
        "step_decompose_transform_gpt3 = StepDecomposeQueryTransform(\n",
        "    llm=llm, verbose=True\n",
        ")\n",
        "\n",
        "query_engine = MultiStepQueryEngine(\n",
        "    query_engine=query_engine_chunk,\n",
        "    query_transform=step_decompose_transform_gpt3,\n",
        "    index_summary=\"Used to answer questions about a paper being submitted to a conference\",\n",
        ")\n",
        "response_gpt35 = query_engine.query(prompt_dict[\"A3_abstract\"])"
      ],
      "metadata": {
        "id": "ldIhEHPbhxIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# query index\n",
        "# gpt-3\n",
        "step_decompose_transform_gpt3 = StepDecomposeQueryTransform(\n",
        "    llm=llm, verbose=True\n",
        ")\n",
        "\n",
        "query_engine = MultiStepQueryEngine(\n",
        "    query_engine=query_engine_chunk,\n",
        "    query_transform=step_decompose_transform_gpt3,\n",
        "    index_summary=\"Used to answer questions about a paper being submitted to a conference\",\n",
        ")\n",
        "response_gpt35 = query_engine.query(prompt_dict[\"A3_abstract\"])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em3ViBXPicET",
        "outputId": "e78c4f83-7fbd-4c07-8c9e-37d8cdffffae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: Behave like you are the author of a paper you are going to submit to a conference.\n",
            "     Do the abstract and introduction summarize the paper’s main claims? If the the answer is 'YES', provide the section name.\n",
            "     If the answer is 'NO' or 'NOT APPLICABLE', provide a justification.\n",
            "     Provide the answer in the first line and provide the section name or justification in the second line.\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGyTR1Swibxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvtFTG4mhw-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "Check documentation here: https://docs.llamaindex.ai/en/stable/examples/retrievers/recurisve_retriever_nodes_braintrust/"
      ],
      "metadata": {
        "id": "WFOq_SfWuZHq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9tmNGUcaI3CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OLD Node Querying"
      ],
      "metadata": {
        "id": "7m0Uu1yZI3Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "query_bundle = QueryBundle(prompt_dict[\"D1_instructions\"])\n",
        "\n",
        "# This is something that should be changed over time\n",
        "vector_retriever_chunk = index.as_retriever(similarity_top_k=40)\n",
        "\n",
        "retriever_chunk = RecursiveRetriever(\n",
        "    \"vector\",\n",
        "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
        "    node_dict=all_nodes_dict,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "retrieved_nodes = retriever_chunk.retrieve(query_bundle)\n",
        "\n",
        "# Need to include this for license and\n",
        "# https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/\n",
        "#postprocessor = KeywordNodePostprocessor(required_keywords=[\"license\"])\n",
        "\n",
        "#retrieved_nodes = postprocessor.postprocess_nodes(retrieved_nodes)\n",
        "\n",
        "#postprocessor = KeywordNodePostprocessor(\n",
        "#    required_keywords=[\"word1\", \"word2\"], exclude_keywords=[\"license\", \"word4\"]\n",
        "#)\n",
        "\n",
        "#postprocessor.postprocess_nodes(nodes)\n",
        "\n",
        "# Current github issue here (rerank is still useful) :\n",
        "# https://github.com/run-llama/llama_index/issues/11093\n",
        "try:\n",
        "    # configure reranker\n",
        "    reranker = LLMRerank(\n",
        "        choice_batch_size = 3,\n",
        "        top_n=2,\n",
        "    )\n",
        "    retrieved_nodes = reranker.postprocess_nodes(\n",
        "        retrieved_nodes, query_bundle\n",
        "    )\n",
        "except:\n",
        "    print('rerank issue')\n",
        "    pass\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JwOIjji-I3NV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}